{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/dmpowell/.cache/huggingface\n",
      "/scratch/dmpowell/.cache/huggingface/datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmpowell/.conda/envs/py3.11transformers4.44/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "## ---------------------------------------------------------------------\n",
    "## set up configs for huggingface hub and OS paths on HPC cluster -- make sure config.ini is correct\n",
    "## ---------------------------------------------------------------------\n",
    "import configparser\n",
    "def auth_token():\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    return config[\"hugging_face\"][\"token\"]\n",
    "\n",
    "def scratch_path():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    return \"/scratch/\" + config[\"user\"][\"username\"] + \"/\"\n",
    "\n",
    "import os\n",
    "if os.path.isdir(scratch_path()):\n",
    "    os.environ['TRANSFORMERS_CACHE'] = scratch_path() + '.cache/huggingface'\n",
    "    os.environ['HF_DATASETS_CACHE'] = scratch_path() + '.cache/huggingface/datasets'\n",
    "print(os.getenv('TRANSFORMERS_CACHE'))\n",
    "print(os.getenv('HF_DATASETS_CACHE'))\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "## Load libraries\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, LlamaForCausalLM, LlamaTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from entailma import * ## these are where the QA and prompting functions live now\n",
    "from easyeditor.custom import EditedModel\n",
    "from easyeditor import LoRAHyperParams, FTHyperParams, BaseEditor\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "## Ensure GPU is available -- device should == 'cuda'\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    MistralForCausalLM,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    GPT2Model,\n",
    "    GPT2LMHeadModel,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "\n",
    "# Imports from the transformer_heads library\n",
    "from transformer_heads import load_headed\n",
    "from transformer_heads.util.helpers import DataCollatorWithPadding, get_model_params\n",
    "from transformer_heads.config import HeadConfig\n",
    "from transformer_heads.util.model import print_trainable_parameters\n",
    "from transformer_heads.util.evaluate import evaluate_head_wise, get_top_n_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = load_dataset(\"wikitext\", \"wikitext-2-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 32000, 'max_position_embeddings': 4096, 'hidden_size': 4096, 'intermediate_size': 11008, 'num_hidden_layers': 32, 'num_attention_heads': 32, 'num_key_value_heads': 32, 'hidden_act': 'silu', 'initializer_range': 0.02, 'rms_norm_eps': 1e-05, 'pretraining_tp': 1, 'use_cache': True, 'rope_theta': 10000.0, 'rope_scaling': None, 'attention_bias': False, 'attention_dropout': 0.0, 'mlp_bias': False, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float16', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['LlamaForCausalLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 1, 'pad_token_id': None, 'eos_token_id': 2, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'meta-llama/Llama-2-7b-hf', 'transformers_version': '4.44.2', 'model_type': 'llama', 'model_class': <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>}\n"
     ]
    }
   ],
   "source": [
    "model_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "train_epochs = 1\n",
    "eval_epochs = 1\n",
    "logging_steps = 100\n",
    "\n",
    "model_params = get_model_params(model_path)\n",
    "model_class = model_params[\"model_class\"]\n",
    "hidden_size = model_params[\"hidden_size\"]\n",
    "vocab_size = model_params[\"vocab_size\"]\n",
    "print(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heads_configs = [\n",
    "#     HeadConfig(\n",
    "#         name=\"wikitext_head\",\n",
    "#         layer_hook=-4,  # Hook to layer [-4] (Drop 3 layers from the end)\n",
    "#         in_size=hidden_size,\n",
    "#         num_layers=1,\n",
    "#         output_activation=\"linear\",\n",
    "#         is_causal_lm=True,\n",
    "#         loss_fct=\"cross_entropy\",\n",
    "#         num_outputs=vocab_size,\n",
    "#         is_regression=False,\n",
    "#         output_bias=False,\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "head_configs = [\n",
    "    HeadConfig(\n",
    "        name=f\"wikitext_head_{i}\",\n",
    "        layer_hook = i,\n",
    "        in_size=hidden_size,\n",
    "        hidden_size=0,\n",
    "        num_layers=1,\n",
    "        output_activation=\"linear\",\n",
    "        is_causal_lm=True,\n",
    "        loss_fct=\"cross_entropy\",\n",
    "        num_outputs=vocab_size,\n",
    "        is_regression=False,\n",
    "        output_bias=False,\n",
    "    )\n",
    "    for i in [3 + i*3 for i in range(0,10)]\n",
    "]\n",
    "head_configs.append(\n",
    "    HeadConfig(\n",
    "        name=f\"lm_head\",\n",
    "        layer_hook=-1,\n",
    "        in_size=hidden_size,\n",
    "        hidden_size=0,\n",
    "        num_layers=1,\n",
    "        output_activation=\"linear\",\n",
    "        is_causal_lm=True,\n",
    "        loss_fct=\"cross_entropy\",\n",
    "        num_outputs=vocab_size,\n",
    "        is_regression=False,\n",
    "        output_bias=False,\n",
    "        trainable=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HeadConfig(name='wikitext_head_3', in_size=4096, num_outputs=32000, layer_hook=3, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_3', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_6', in_size=4096, num_outputs=32000, layer_hook=6, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_6', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_9', in_size=4096, num_outputs=32000, layer_hook=9, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_9', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_12', in_size=4096, num_outputs=32000, layer_hook=12, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_12', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_15', in_size=4096, num_outputs=32000, layer_hook=15, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_15', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_18', in_size=4096, num_outputs=32000, layer_hook=18, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_18', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_21', in_size=4096, num_outputs=32000, layer_hook=21, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_21', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_24', in_size=4096, num_outputs=32000, layer_hook=24, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_24', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_27', in_size=4096, num_outputs=32000, layer_hook=27, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_27', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_30', in_size=4096, num_outputs=32000, layer_hook=30, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_30', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='lm_head', in_size=4096, num_outputs=32000, layer_hook=-1, hidden_size=0, num_layers=1, output_activation='linear', target='lm_head', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=False, loss_weight=1.0, ignore_pads=False, block_gradients=False)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ccf6c975ea4fb3a0b548162227bf65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2870 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e298953337a449bbfab6e733d4bcffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb73238b16348aa8cc64e717eee785c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    out = tokenizer(examples[\"text\"], padding=False, truncation=True)\n",
    "    for hc in head_configs:\n",
    "        out[hc.name] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "\n",
    "for split in dd.keys():\n",
    "    dd[split] = dd[split].filter(function=lambda example: len(example[\"text\"]) > 10)\n",
    "    dd[split] = dd[split].map(tokenize_function, batched=True)\n",
    "dd.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\"] + [x.name for x in head_configs],\n",
    ")\n",
    "for split in dd.keys():\n",
    "    dd[split] = dd[split].remove_columns(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'wikitext_head_3', 'wikitext_head_6', 'wikitext_head_9', 'wikitext_head_12', 'wikitext_head_15', 'wikitext_head_18', 'wikitext_head_21', 'wikitext_head_24', 'wikitext_head_27', 'wikitext_head_30', 'lm_head'],\n",
       "    num_rows: 23627\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbedf41e75ff4be5b62d1f98d766f980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efea9f481fc4360b266ae3fe8342cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TransformerWithHeads were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['heads.wikitext_head_12.lins.0.weight', 'heads.wikitext_head_15.lins.0.weight', 'heads.wikitext_head_18.lins.0.weight', 'heads.wikitext_head_21.lins.0.weight', 'heads.wikitext_head_24.lins.0.weight', 'heads.wikitext_head_27.lins.0.weight', 'heads.wikitext_head_3.lins.0.weight', 'heads.wikitext_head_30.lins.0.weight', 'heads.wikitext_head_6.lins.0.weight', 'heads.wikitext_head_9.lins.0.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     load_in_8bit=False,\n",
    "#     llm_int8_threshold=6.0,\n",
    "#     llm_int8_has_fp16_weight=False,\n",
    "#     bnb_4bit_compute_dtype=torch.float32,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "# )\n",
    "\n",
    "model = load_headed(\n",
    "    model_class,\n",
    "    model_path,\n",
    "    head_configs=head_configs,\n",
    "    # quantization_config=quantization_config,\n",
    "    device_map={\"\": torch.cuda.current_device()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all params: 8049135616 || trainable params: 1310720000 || trainable%: 16.28398454853416\n",
      "params by dtype: defaultdict(<class 'int'>, {torch.float32: 8049135616})\n",
      "trainable params by dtype: defaultdict(<class 'int'>, {torch.float32: 1310720000})\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HeadConfig(name='wikitext_head_3', in_size=4096, num_outputs=32000, layer_hook=3, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_3', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_6', in_size=4096, num_outputs=32000, layer_hook=6, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_6', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_9', in_size=4096, num_outputs=32000, layer_hook=9, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_9', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_12', in_size=4096, num_outputs=32000, layer_hook=12, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_12', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_15', in_size=4096, num_outputs=32000, layer_hook=15, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_15', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_18', in_size=4096, num_outputs=32000, layer_hook=18, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_18', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_21', in_size=4096, num_outputs=32000, layer_hook=21, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_21', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_24', in_size=4096, num_outputs=32000, layer_hook=24, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_24', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_27', in_size=4096, num_outputs=32000, layer_hook=27, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_27', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='wikitext_head_30', in_size=4096, num_outputs=32000, layer_hook=30, hidden_size=0, num_layers=1, output_activation='linear', target='wikitext_head_30', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=True, loss_weight=1.0, ignore_pads=False, block_gradients=False),\n",
       " HeadConfig(name='lm_head', in_size=4096, num_outputs=32000, layer_hook=-1, hidden_size=0, num_layers=1, output_activation='linear', target='lm_head', is_causal_lm=True, pred_for_sequence=False, is_regression=False, output_bias=False, loss_fct='cross_entropy', trainable=False, loss_weight=1.0, ignore_pads=False, block_gradients=False)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def get_top_n_preds(\n",
    "    n,\n",
    "    model,\n",
    "    text,\n",
    "    tokenizer,\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the top n predictions for a given text. Use for models with causal language modeling heads.\n",
    "\n",
    "    Args:\n",
    "        n (int): The number of top predictions to get.\n",
    "        model (HeadedModel): The model to be used for prediction.\n",
    "        text (str): The input text to be used for prediction.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer to be used.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: The top n predictions for each head.\n",
    "    \"\"\"\n",
    "    input = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    output = model(**input)\n",
    "    out = {}\n",
    "    for head_name in output.preds_by_head:\n",
    "        logits = output.preds_by_head[head_name]\n",
    "        pred_logits = logits[0, -1, :]\n",
    "        best_n = torch.topk(pred_logits, n)\n",
    "        out[head_name] = [tokenizer.decode(i) for i in best_n.indices]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_head_weights(model, heads_path):\n",
    "    for head_name, module in model.heads.items():\n",
    "        torch.save(module.state_dict(), f'{heads_path}/{head_name}.pt')\n",
    "\n",
    "\n",
    "def load_head_weights(model, heads_path):\n",
    "    # model must be initialized with corresponding configs\n",
    "    for head_name in model.heads.keys():\n",
    "        model.heads[head_name].load_state_dict(torch.load(f'{heads_path}/{head_name}.pt', weights_only=True))\n",
    "\n",
    "# save_head_weights(model, 'linear_probes/llama-wiki')\n",
    "# load_head_weights(model, 'linear_probes/llama-wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wikitext_head_3': ['tit', 'кре', 'través', 'RU', 'Ligações'], 'wikitext_head_6': ['готов', 'ubuntu', 'tijdens', 'дере', 'typed'], 'wikitext_head_9': ['хо', 'ürgen', 'Current', 'ح', 'web'], 'wikitext_head_12': ['cha', 'чь', 'Collins', 'quantity', '($'], 'wikitext_head_15': ['ür', 'Issue', 'irty', 'Bahnhof', 'Ont'], 'wikitext_head_18': ['eurs', 'ét', 'Neg', 'appen', 'attend'], 'wikitext_head_21': ['Си', 'Sym', 'laravel', 'food', 'cs'], 'wikitext_head_24': ['perm', 'Blo', '只', 'iego', '}^{-'], 'wikitext_head_27': ['ิ', 'aside', 'raise', 'Iowa', 'academic'], 'wikitext_head_30': ['rypt', '；', 'vement', 'ensemble', 'ких'], 'lm_head': ['the', 'this', 'a', '', 'The']}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    get_top_n_preds(\n",
    "        n=5, model=model, text=\"The historical significance of\", tokenizer=tokenizer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/26/2024 17:48:35 - WARNING - accelerate.utils.other -   Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2954' max='2954' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2954/2954 2:24:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>91.687300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>65.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>57.118400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>53.938100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>51.267900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>49.203800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>47.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>46.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>46.346500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>44.983700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>44.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>43.907100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>43.550300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>42.799900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>42.379900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>42.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>41.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>41.798100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>41.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>41.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>40.956200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>40.917200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>40.871500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>39.964800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>40.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>39.774800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>40.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>39.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>40.006600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2954, training_loss=46.171842106307125, metrics={'train_runtime': 8685.148, 'train_samples_per_second': 2.72, 'train_steps_per_second': 0.34, 'total_flos': 3.316235643950776e+17, 'train_loss': 46.171842106307125, 'epoch': 1.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOAD_DATA = True\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{scratch_path()}model_checkpoints/linear_probe_test\",\n",
    "    learning_rate=0.0002,\n",
    "    num_train_epochs=train_epochs,\n",
    "    logging_steps=logging_steps,\n",
    "    do_eval=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "collator = DataCollatorWithPadding(\n",
    "    feature_name_to_padding_value={\n",
    "        \"input_ids\": tokenizer.pad_token_id,\n",
    "        \"attention_mask\": 0,\n",
    "        **{key.name: -100 for key in head_configs},\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args=args,\n",
    "    train_dataset=dd[\"train\"],\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "if LOAD_DATA:\n",
    "    load_head_weights(model, 'linear_probes/llama-wiki')\n",
    "else: \n",
    "    trainer.train()\n",
    "    save_head_weights(model, 'linear_probes/llama-wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_head_wise(\n",
    "    model, ds, collator=None, batch_size=8, epochs=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the model loss for each of its heads.\n",
    "\n",
    "    Args:\n",
    "        model (HeadedModel): The model to be evaluated.\n",
    "        ds (Dataset): The dataset to be used for evaluation.\n",
    "        collator (callable, optional): Merges a list of samples to form a mini-batch.\n",
    "        batch_size (int, optional): The size of each batch. Defaults to 8.\n",
    "        epochs (int, optional): The number of epochs for evaluation. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        tuple[int, dict[str, int]]: The overall loss and the losses by each head.\n",
    "    \"\"\"\n",
    "    ds = ds.with_format(type=\"torch\")\n",
    "    loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, collate_fn=collator)\n",
    "    losses_by_head = defaultdict(list)\n",
    "    losses = []\n",
    "    \n",
    "    for i, batch in tqdm(\n",
    "        enumerate(loader), total=len(loader) * epochs, desc=\"Evaluating\"\n",
    "    ):\n",
    "        \n",
    "        for k in batch.keys():\n",
    "            batch[k] = batch[k].to(device)\n",
    "            \n",
    "        outputs  = model(**batch)\n",
    "\n",
    "        for key in outputs.loss_by_head:\n",
    "            losses_by_head[key].append(float(outputs.loss_by_head[key].item()))\n",
    "        losses.append(float(outputs.loss.item()))\n",
    "        if i >= len(loader) * epochs:\n",
    "            break\n",
    "    losses = float(np.mean(losses))\n",
    "    losses_by_head = {\n",
    "        key: float(np.mean(losses_by_head[key])) for key in losses_by_head\n",
    "    }\n",
    "    return losses, losses_by_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 308/308 [10:25<00:00,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39.97310024732119, {'wikitext_head_3': 5.18283951127684, 'wikitext_head_6': 4.525789303439004, 'wikitext_head_9': 4.180435857215485, 'wikitext_head_12': 3.9740616508892606, 'wikitext_head_15': 3.6247242178235735, 'wikitext_head_18': 3.227270274193256, 'wikitext_head_21': 2.9902305812030643, 'wikitext_head_24': 2.9098655802088897, 'wikitext_head_27': 2.8812276078509047, 'wikitext_head_30': 2.883846577885863, 'lm_head': 3.592809163130723})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_head_wise(model, dd[\"validation\"], collator, epochs=eval_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wikitext_head_3': ['a', 'the', 'also', 'not', 'an'], 'wikitext_head_6': ['a', 'the', 'in', 'also', 'is'], 'wikitext_head_9': ['', 'is', 'a', 'the', 'located'], 'wikitext_head_12': ['is', 'also', 'the', 'a', 'located'], 'wikitext_head_15': ['located', 'the', 'a', 'also', ''], 'wikitext_head_18': ['the', 'a', '', 'one', 'also'], 'wikitext_head_21': ['Paris', 'also', 'the', 'situated', 'London'], 'wikitext_head_24': ['Paris', 'also', 'situated', 'France', 'known'], 'wikitext_head_27': ['a', 'the', 'home', 'Paris', 'known'], 'wikitext_head_30': ['also', 'a', 'home', 'the', 'not'], 'lm_head': ['a', 'Paris', 'the', 'one', 'known']}\n"
     ]
    }
   ],
   "source": [
    "print(get_top_n_preds(5, model, \"The capital city of France is\", tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wikitext_head_3': tensor([-9.1976], device='cuda:0'),\n",
       " 'wikitext_head_6': tensor([-8.9075], device='cuda:0'),\n",
       " 'wikitext_head_9': tensor([-8.3670], device='cuda:0'),\n",
       " 'wikitext_head_12': tensor([-8.4271], device='cuda:0'),\n",
       " 'wikitext_head_15': tensor([-7.3288], device='cuda:0'),\n",
       " 'wikitext_head_18': tensor([-5.7584], device='cuda:0'),\n",
       " 'wikitext_head_21': tensor([-1.2945], device='cuda:0'),\n",
       " 'wikitext_head_24': tensor([-1.2887], device='cuda:0'),\n",
       " 'wikitext_head_27': tensor([-2.9376], device='cuda:0'),\n",
       " 'wikitext_head_30': tensor([-6.8673], device='cuda:0'),\n",
       " 'lm_head': tensor([-2.2456], device='cuda:0')}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def head_logits(input_text, model, tokenizer):\n",
    "\n",
    "    input_ids = tokenizer(input_text, return_tensors = 'pt')['input_ids'].to(device)\n",
    "    outputs  = model(input_ids)\n",
    "\n",
    "    return outputs.preds_by_head\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def logprob_observed_by_head(input_text, model, tokenizer):\n",
    "\n",
    "    input_ids = tokenizer(input_text, return_tensors = 'pt')['input_ids'].to(device)\n",
    "    logits_by_head = head_logits(input_text, model, tokenizer)\n",
    "\n",
    "    ol_dict = {}\n",
    "    \n",
    "\n",
    "    if type(input_text) is str:\n",
    "        tok_idx = input_ids.squeeze()\n",
    "        for k, v in logits_by_head.items():\n",
    "            logits = F.log_softmax(v, -1)\n",
    "            ol_dict[k] =  logits[0, :, tok_idx[1:]].squeeze().diag()\n",
    "\n",
    "    return ol_dict\n",
    "\n",
    "\n",
    "def logprob_observed_by_head_sub(input_text, subtext, model, tokenizer):\n",
    "    input_ids = tokenizer(input_text, return_tensors = 'pt')['input_ids'][0]\n",
    "    sub_ids = tokenizer(subtext, return_tensors = 'pt')['input_ids'][0][1:]\n",
    "    logits_by_head = logprob_observed_by_head(input_text, model, tokenizer)\n",
    "    \n",
    "    \n",
    "    for i in reversed(list(range(0, len(input_ids) - len(sub_ids) + 1))):\n",
    "        # going backwards, get first matching completion\n",
    "        idx = i\n",
    "        idx1 = idx + len(sub_ids)\n",
    "        \n",
    "        if input_ids[idx:idx1] == sub_ids:\n",
    "            out = {k: v[idx-1:idx1-1] for k, v in logits_by_head.items()}\n",
    "            break\n",
    "\n",
    "    return(out)\n",
    "\n",
    "\n",
    "\n",
    "logprob_observed_by_head_sub('The capital of France is Paris', 'Paris', model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logprob_observed_by_head' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logprob_observed_by_head(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe capial of France is Paris\u001b[39m\u001b[38;5;124m'\u001b[39m, model, tokenizer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logprob_observed_by_head' is not defined"
     ]
    }
   ],
   "source": [
    "logprob_observed_by_head('The capial of France is Paris', model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3.5791]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[-11.2749, -10.4908,  -9.6274,  -3.2569,  -5.0563,  -1.9826,  -3.5791][6:7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11transformers4.44",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
