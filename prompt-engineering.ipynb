{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6bdf34-8665-432b-a944-13ebe977aada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/dmpowell/.cache/huggingface\n",
      "/scratch/dmpowell/.cache/huggingface/datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "## ---------------------------------------------------------------------\n",
    "## set up configs for huggingface hub and OS paths on HPC cluster -- make sure config.ini is correct\n",
    "## ---------------------------------------------------------------------\n",
    "import configparser\n",
    "def auth_token():\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    return config[\"hugging_face\"][\"token\"]\n",
    "\n",
    "def scratch_path():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    return \"/scratch/\" + config[\"user\"][\"username\"] + \"/\"\n",
    "\n",
    "import os\n",
    "if os.path.isdir(scratch_path()):\n",
    "    os.environ['TRANSFORMERS_CACHE'] = scratch_path() + '.cache/huggingface'\n",
    "    os.environ['HF_DATASETS_CACHE'] = scratch_path() + '.cache/huggingface/datasets'\n",
    "print(os.getenv('TRANSFORMERS_CACHE'))\n",
    "print(os.getenv('HF_DATASETS_CACHE'))\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "## Load libraries\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from entailma import * ## these are where the QA and prompting functions live now\n",
    "from easyeditor.custom import EditedModel\n",
    "from easyeditor import LoRAHyperParams, FTHyperParams, BaseEditor\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "## Ensure GPU is available -- device should == 'cuda'\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f6c525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EditedModel:\n",
    "    def __init__(self, hparams, auth_token=None):\n",
    "        \n",
    "        self.editor = BaseEditor.from_hparams(hparams)\n",
    "\n",
    "        self.model = self.editor.model\n",
    "        self.tok = self.editor.tok\n",
    "        self.model_name = self.editor.model_name\n",
    "\n",
    "        self.params = hparams\n",
    "        self.preprompt = \"\"\n",
    "        self.saved_weights = None\n",
    "        \n",
    "        if type(self.tok) == transformers.LlamaTokenizer or transformers.LlamaTokenizerFast:\n",
    "            self.tok.padding_side = \"right\"\n",
    "        else: \n",
    "            self.tok.padding_side = \"left\"\n",
    "    \n",
    "    def edit(self, rewrite, log_file = None, **kwargs):\n",
    "        if log_file:\n",
    "            h = open(log_file, \"a\")\n",
    "        else:\n",
    "            h = None\n",
    "        \n",
    "        if \"preprompt\" in rewrite: # this is a little hacky\n",
    "            self.preprompt = rewrite[\"preprompt\"]\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            with redirect_stdout(h): # None\n",
    "                metrics, self.model, self.saved_weights = self.editor.pure_edit( # pure_edit\n",
    "                    **rewrite,\n",
    "                    # **kwargs,\n",
    "                    keep_original_weight = True,\n",
    "                    verbose = False\n",
    "                )\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def restore(self):\n",
    "\n",
    "        self.preprompt = \"\"\n",
    "        \n",
    "        if self.params.alg_name == \"LoRA\":\n",
    "            self.model = self.model.unload()\n",
    "        \n",
    "        elif self.saved_weights:\n",
    "\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    for k, v in self.saved_weights.items():\n",
    "                        nethook.get_parameter(self.model, k)[...] = v\n",
    "                self.saved_weights = None\n",
    "                # print(\"Original model restored\")\n",
    "            except NameError as e:\n",
    "                print(f\"No model weights to restore: {e}\")\n",
    "\n",
    "        elif self.saved_weights == {}:\n",
    "            print (print(f\"No model weights to restore: saved_weights is empty dict\"))\n",
    "\n",
    "        return None\n",
    "\n",
    "            \n",
    "    def generate_text(self, texts, **kwargs):\n",
    "        \n",
    "        if type(texts) != list:\n",
    "            texts = [texts]\n",
    "        \n",
    "        texts = [self.preprompt + t for t in texts]\n",
    "\n",
    "        model = self.model\n",
    "        tokenizer = self.tok\n",
    "        encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**encoding, **kwargs) # \n",
    "\n",
    "            generated_texts = tokenizer.batch_decode(\n",
    "                generated_ids, skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "        return(generated_texts)\n",
    "    \n",
    "    \n",
    "    # def logprobs(self, texts):\n",
    "        \n",
    "    #     # texts = self.preprompt + texts if type(texts)==str else [self.preprompt + t for t in texts]\n",
    "    \n",
    "    #     # tokenizer = self.tok \n",
    "    #     # model = self.model\n",
    "    #     # encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "    #     # with torch.no_grad():\n",
    "    #     #     model_out = model(encoding[\"input_ids\"])\n",
    "    #     #     logits = model_out.logits\n",
    "    #     #     logprobs = F.log_softmax(logits, -1)\n",
    "\n",
    "    #     x = self.logits(texts)\n",
    "        \n",
    "    #     return {\"tokens\": x['tokens'], \"logprobs\": logprobs}\n",
    "    \n",
    "\n",
    "    def logits(self, texts):\n",
    "        \n",
    "        texts = self.preprompt + texts if type(texts)==str else [self.preprompt + t for t in texts]\n",
    "    \n",
    "        tokenizer = self.tok \n",
    "        model = self.model\n",
    "        encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_out = model(encoding[\"input_ids\"])\n",
    "            logits = model_out.logits\n",
    "        \n",
    "        return {\"tokens\": encoding, \"logits\": logits}\n",
    "    \n",
    "    \n",
    "    def logprobs(self, texts):\n",
    "        \n",
    "        logits = self.logits(texts)\n",
    "        \n",
    "        return {\"tokens\": logits['tokens'], \"logprobs\": F.log_softmax(logits['logits'], -1)}\n",
    "    \n",
    "    \n",
    "    def obs_logits(self, text):\n",
    "    \n",
    "        x = self.logits(text)\n",
    "        logits = x['logits']\n",
    "        \n",
    "        obslogits = []\n",
    "\n",
    "        if type(text) is str:\n",
    "            tok_idx = x['tokens']['input_ids'].squeeze()\n",
    "            logits = x['logits']\n",
    "            obslogits = logits[0, :, tok_idx[1:]].squeeze().diag()\n",
    "\n",
    "        elif type(text) is list:\n",
    "            for i in range(len(text)):\n",
    "                tok_idx = x['tokens']['input_ids'][i].squeeze()\n",
    "                mask = x['tokens']['attention_mask'][i] > 0\n",
    "                \n",
    "                obslogits.append(logits[0, :, tok_idx[1:]].squeeze().diag()[mask[1:]])\n",
    "\n",
    "        return obslogits\n",
    "\n",
    "\n",
    "    def obs_logprobs(self, text):\n",
    "        logits = self.obs_logits(text)\n",
    "\n",
    "        return [F.log_softmax(l, -1) for l in logits] if type(logits)==list else F.log_softmax(logits, -1)\n",
    "        \n",
    "       \n",
    "    def completion_logprob(self, text, completion, start_ind = None):\n",
    "        \n",
    "        '''\n",
    "        Compute model log probability of completion substring. Returns single value tensor. Takes only one text string.\n",
    "        '''\n",
    "\n",
    "        return self.substring_logprobs(text, completion)[0][-1]\n",
    "        \n",
    "\n",
    "    def substring_logprobs(self, texts, substring, pad = True):\n",
    "        '''\n",
    "        Compute model log probability of each occurrence of substring in text. Returns list of list-type. Accepts a list of strings.\n",
    "        '''\n",
    "        \n",
    "        if type(texts) != list:\n",
    "            texts = [texts]\n",
    "        \n",
    "        logprobs = self.logprobs(texts)\n",
    "        \n",
    "        tok_encoded = encode_token(substring, self.tok, pad = pad)\n",
    "        # text_encoded = logprobs['tokens']['input_ids'][0].tolist()\n",
    "        \n",
    "        out = []\n",
    "        for i in range(len(texts)):\n",
    "            text_encoded = logprobs['tokens']['input_ids'][i].tolist()\n",
    "\n",
    "            # find matches for searched token sequence\n",
    "            start_idxs = []\n",
    "            for left in range(0, len(text_encoded) - len(tok_encoded)+1):\n",
    "                # left = i - 1\n",
    "                right = left + len(tok_encoded)\n",
    "                if text_encoded[left:right] == tok_encoded:\n",
    "                    start_idxs.append(left)\n",
    "\n",
    "            lp = logprobs['logprobs'][i]\n",
    "            match_probs = []\n",
    "\n",
    "            # compute probability for all tokens\n",
    "            for start in start_idxs:\n",
    "                val = 0\n",
    "                for i in range(len(tok_encoded)):\n",
    "                    val += lp[start + i - 1][tok_encoded[i]]\n",
    "                match_probs.append(val)\n",
    "\n",
    "            out.append(match_probs)\n",
    "\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def choose(self, prompt, choices, normalization = None):\n",
    "\n",
    "        # prompt = prompt.rstrip() # remove any trailing whitespace\n",
    "\n",
    "        if type(self.tok) == transformers.models.llama.tokenization_llama.LlamaTokenizer:\n",
    "            padded_choices = choices\n",
    "            prompt = prompt + \" \" if prompt[-1]!= \" \" else prompt\n",
    "        else:\n",
    "            padded_choices = [pad_token(c) for c in choices] # pad all the \n",
    "        \n",
    "        prompts = [prompt + c for c in padded_choices]\n",
    "\n",
    "        logits = torch.tensor([self.completion_logprob(prompts[i], padded_choices[i]) for i in range(len(padded_choices))])\n",
    "\n",
    "        if normalization == \"unconditional\":\n",
    "            norm_logits = torch.tensor([self.completion_logprob(padded_choices[i], padded_choices[i]) for i in range(len(padded_choices))])\n",
    "            logits = logits - norm_logits\n",
    "\n",
    "        elif normalization == \"byte_length\":    \n",
    "            str_lens = [len(c) for c in choices]\n",
    "            logits = logits / torch.tensor(str_lens)\n",
    "\n",
    "        elif normalization == \"token_length\":\n",
    "            tok_lens = [len(encode_token(c, self.tok)) for c in choices]\n",
    "            logits = logits / torch.tensor(tok_lens)\n",
    "\n",
    "        elif normalization == \"root\":\n",
    "            tok_lens = [len(encode_token(c, self.tok)) for c in choices]\n",
    "            logits = torch.pow(torch.exp(logits), 1./torch.tensor(tok_lens))\n",
    "\n",
    "        logits = logits.tolist()\n",
    "\n",
    "        return(logits.index(max(logits)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68833b8a-c4f5-48dd-8153-2c7ad99b7f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 12:53:17,645 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "08/10/2024 12:53:17 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004730939865112305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 73,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b9394987fa4975a96e22d5e44afcd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "2024-08-10 12:54:24,764 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "08/10/2024 12:54:24 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to left...\n"
     ]
    }
   ],
   "source": [
    "## ---------------------------------------------------------------------\n",
    "## load llama-2 as a EditedModel class (not pipeline, to integrate better with other scripts/notebooks)\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\" \n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = LlamaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map = \"auto\")\n",
    "\n",
    "hparams = FTHyperParams.from_hparams('hparams/FT/llama-7b.yaml')\n",
    "model = EditedModel(hparams, auth_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abbf1d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EditedModel at 0x15540c616700>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a29eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question_stem</th>\n",
       "      <th>choices</th>\n",
       "      <th>complete_question</th>\n",
       "      <th>answer_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>9-743</td>\n",
       "      <td>where might a bunny live?</td>\n",
       "      <td>(A) a thicket (B) atop palm trees (C) a sewer ...</td>\n",
       "      <td>where might a bunny live? (A) a thicket (B) at...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>9-645</td>\n",
       "      <td>A shark will be unable to survive on eating al...</td>\n",
       "      <td>(A) it is a predator (B) it is a vegetarian (C...</td>\n",
       "      <td>A shark will be unable to survive on eating al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>8-250</td>\n",
       "      <td>A meadow vole just gave birth, and needs to fe...</td>\n",
       "      <td>(A) oil (B) deer (C) bugs (D) recycled plastic...</td>\n",
       "      <td>A meadow vole just gave birth, and needs to fe...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>283</td>\n",
       "      <td>The Grand Canyon was formed by</td>\n",
       "      <td>(A) a volcano erupting in 1782 (B) a river nam...</td>\n",
       "      <td>The Grand Canyon was formed by (A) a volcano e...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>8-183</td>\n",
       "      <td>A woman, with a pale complexion, wants to spen...</td>\n",
       "      <td>(A) UV rays are harmful (B) sunlight will be f...</td>\n",
       "      <td>A woman, with a pale complexion, wants to spen...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                      question_stem  \\\n",
       "490  9-743                          where might a bunny live?   \n",
       "491  9-645  A shark will be unable to survive on eating al...   \n",
       "492  8-250  A meadow vole just gave birth, and needs to fe...   \n",
       "493    283                     The Grand Canyon was formed by   \n",
       "494  8-183  A woman, with a pale complexion, wants to spen...   \n",
       "\n",
       "                                               choices  \\\n",
       "490  (A) a thicket (B) atop palm trees (C) a sewer ...   \n",
       "491  (A) it is a predator (B) it is a vegetarian (C...   \n",
       "492  (A) oil (B) deer (C) bugs (D) recycled plastic...   \n",
       "493  (A) a volcano erupting in 1782 (B) a river nam...   \n",
       "494  (A) UV rays are harmful (B) sunlight will be f...   \n",
       "\n",
       "                                     complete_question answer_key  \n",
       "490  where might a bunny live? (A) a thicket (B) at...          A  \n",
       "491  A shark will be unable to survive on eating al...          A  \n",
       "492  A meadow vole just gave birth, and needs to fe...          C  \n",
       "493  The Grand Canyon was formed by (A) a volcano e...          C  \n",
       "494  A woman, with a pale complexion, wants to spen...          A  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/obqa/test.tsv\", sep='\\t')\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df2 = df.copy().tail(10) # smaller df for testing\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f95ce1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8646, -2.9641, -3.7612, -3.6786], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mc_choose_answer(question, model, tokenizer=None):\n",
    "    if not tokenizer:\n",
    "        tokenizer = model.tok\n",
    "    \n",
    "    input_str = mc_answer_prompt + f\"\\nQuestion: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    with torch.no_grad():\n",
    "        sequences = model.generate(input_ids = input_ids, max_new_tokens = 1)\n",
    "    \n",
    "    return tokenizer.decode(sequences[0])[-1]\n",
    "\n",
    "\n",
    "def last_token_logprobs(text, last_tokens, model):\n",
    "    x = model.logprobs(text)\n",
    "    logprobs = x['logprobs']\n",
    "    t_idx = [i[-1] for i in model.tok(last_tokens)['input_ids']]\n",
    "\n",
    "    return(logprobs[0, -1, t_idx])\n",
    "\n",
    "\n",
    "def mc_answer_logprobs(question, model, answers = ['A','B','C','D']):\n",
    "\n",
    "    input_str = mc_answer_prompt + f\"\\n\\nQuestion: {question}\\nAnswer: \"\n",
    "\n",
    "    return last_token_logprobs(input_str, answers, model)\n",
    "\n",
    "\n",
    "mc_answer_logprobs('What color is the sky? (A) blue (B) red (C) orange (D) black', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a58813",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc72891a",
   "metadata": {},
   "source": [
    "Question answering is getting ~58% accuracy. For reference, the original GPT-3 with 32-shot examples got 65.8% ([Brown et al., 2020](https://arxiv.org/abs/2005.14165v4)). So that seems not-too-bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e2e8c-053d-4f9a-8e1d-3f125b4dc5ac",
   "metadata": {},
   "source": [
    "## generate_premises() function\n",
    "~~This function will read the model's statement from the data set and provide two premises that would make the statement true.~~\n",
    "\n",
    "UPDATE: This seems to work better if we include the original question and answer, which eliminates a point of failure and gives more context for the explanation / premise generation.\n",
    "\n",
    "UPDATE 2: This is in the `entailma` library in this repo, but I've reproduced it here to make it easier to play around with as you/we tweak prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920cc16e",
   "metadata": {},
   "source": [
    "## updates:\n",
    "\n",
    "\n",
    "- Need a way to score whether the premises are actually any \"good\" -- i.e. do they lead the model to choose the targeted answer? The code below implements an IKE/ICE-style version of this. It seems to work ok?\n",
    "- Need to add more examples to the prompt of premises supportin INCORRECT answers, as it struggles with this ATM [quick and dirty version done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a7d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4303, device='cuda:0')\n",
      "tensor(1.5171, device='cuda:0')\n",
      "tensor(1.2276, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def score_premises(premises, question, target_answer, model, answers = ['A','B','C','D']):\n",
    "   '''Returns the odds-ratio of the target answer with vs without the premises in the premises in the context.'''\n",
    "   \n",
    "   reg_answer_prompt = mc_answer_prompt +  \"\\n\\nQuestion:\" + question+\"\\nAnswer: \"\n",
    "   logprobs0 = last_token_logprobs(reg_answer_prompt, answers, model)\n",
    "   prob0 = logprobs0[answers.index(target_answer)].exp() / logprobs0.exp().sum()\n",
    "\n",
    "   premise_str = \"\\n\".join(premises)\n",
    "   augmented_answer_prompt = mc_answer_prompt + '\\n\\n' + premise_str + '\\nQuestion: ' + question  + 'Answer: '\n",
    "   logprobs1 = last_token_logprobs(augmented_answer_prompt,  answers, model)\n",
    "   prob1 = logprobs1[answers.index(target_answer)].exp() / logprobs1.exp().sum()\n",
    "\n",
    "   return( (prob1/(1-prob1)) / (prob0/(1-prob0)))\n",
    "\n",
    "\n",
    "print(score_premises(['The sky is red.', 'At sunset, the sun can be extremely red.'], 'What color is the sky? (A) blue (B) red (C) yellow (D) black', 'B', model))\n",
    "print(score_premises(['Some things are red.', 'My favorite color is red.'], 'What color is the sky? (A) blue (B) red (C) yellow (D) black', 'B', model))\n",
    "print(score_premises(['red red red red.', 'red red red red red.'], 'What color is the sky? (A) blue (B) red (C) yellow (D) black', 'B', model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db215361-322c-42e9-a759-2ceb1c6d64c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 580.00 MiB (GPU 0; 79.26 GiB total capacity; 76.53 GiB already allocated; 268.75 MiB free; 78.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m premises[max_idx]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df2\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitertuples():\n\u001b[0;32m---> 48\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_best_premises\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_prem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(out)\n",
      "Cell \u001b[0;32mIn[12], line 38\u001b[0m, in \u001b[0;36mgenerate_best_premises\u001b[0;34m(question, answer, model, num_prem)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_best_premises\u001b[39m(question, answer, model, num_prem\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 38\u001b[0m     premises \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_premises\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_prem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     scores \u001b[38;5;241m=\u001b[39m [score_premises(p, question, answer, model) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m premises]\n\u001b[1;32m     41\u001b[0m     max_idx \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmax\u001b[39m(scores))\n",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m, in \u001b[0;36mgenerate_premises\u001b[0;34m(question, answer, model, num_prem)\u001b[0m\n\u001b[1;32m      7\u001b[0m input_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpremises_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m pipe \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# use_auth_token = auth_token()\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m sequences \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_prem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m generated_texts \u001b[38;5;241m=\u001b[39m [s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sequences]\n\u001b[1;32m     29\u001b[0m premises \u001b[38;5;241m=\u001b[39m [t[\u001b[38;5;28mlen\u001b[39m(input_str):\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m generated_texts]\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:201\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/transformers/pipelines/base.py:1120\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1114\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         )\n\u001b[1;32m   1118\u001b[0m     )\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/transformers/pipelines/base.py:1127\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1126\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1127\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1128\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/transformers/pipelines/base.py:1026\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1025\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1026\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:263\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/transformers/generation/utils.py:1572\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1564\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1565\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1566\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1567\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1568\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1569\u001b[0m     )\n\u001b[1;32m   1571\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/transformers/generation/utils.py:2619\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2618\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2619\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2620\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2627\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    701\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:578\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    570\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    571\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    572\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    576\u001b[0m     )\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:292\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    289\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:212\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    208\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    210\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m (key_states, value_states) \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_weights\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, kv_seq_len):\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention weights should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mq_len,\u001b[38;5;250m \u001b[39mkv_seq_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_weights\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 580.00 MiB (GPU 0; 79.26 GiB total capacity; 76.53 GiB already allocated; 268.75 MiB free; 78.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "with open(\"entailma/prompt3b.txt\", 'r') as file:\n",
    "    premises_prompt = file.read()\n",
    "    \n",
    "\n",
    "def generate_premises(question, answer, model, num_prem = 1):\n",
    "    \n",
    "    input_str = f\"\\n\\n{premises_prompt}Question: {question}\\nAnswer: {answer}\\n\"\n",
    "\n",
    "    pipe = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model = model.model,\n",
    "        tokenizer = model.tok,\n",
    "        torch_dtype=torch.float16,\n",
    "        # device_map=\"cuda\",\n",
    "        device = model.model.device,\n",
    "        # use_auth_token = auth_token()\n",
    "    )\n",
    "\n",
    "    sequences = pipe(\n",
    "        input_str,\n",
    "        top_p=.5,\n",
    "        temperature = .7,\n",
    "        max_new_tokens = 75,\n",
    "        num_return_sequences = num_prem,\n",
    "        batch_size = 4\n",
    "    )\n",
    "    \n",
    "    generated_texts = [s['generated_text'] for s in sequences]\n",
    "    premises = [t[len(input_str):-1] for t in generated_texts]\n",
    "    premlist = [p.split(\".\\n\")[:2] for p in premises] \n",
    "\n",
    "    return premlist if len(premlist) > 1 else premlist[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_best_premises(question, answer, model, num_prem=10):\n",
    "    premises = generate_premises(question, answer, model, num_prem)\n",
    "\n",
    "    scores = [score_premises(p, question, answer, model) for p in premises]\n",
    "    max_idx = scores.index(max(scores))\n",
    "    print(max(scores))\n",
    "\n",
    "    return premises[max_idx]\n",
    "\n",
    "\n",
    "for row in df2.tail(1).itertuples():\n",
    "    out = generate_best_premises(row.complete_question, row.answer_key, model, num_prem = 2)\n",
    "    print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13485b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3655, device='cuda:0')\n",
      "tensor(1.1231, device='cuda:0')\n",
      "tensor(1.1388, device='cuda:0')\n",
      "tensor(1.1388, device='cuda:0')\n",
      "tensor(1.2130, device='cuda:0')\n",
      "tensor(1.3499, device='cuda:0')\n",
      "tensor(1.6226, device='cuda:0')\n",
      "tensor(1.2164, device='cuda:0')\n",
      "tensor(1.1155, device='cuda:0')\n",
      "tensor(1.5467, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for ps in out:\n",
    "    print(score_premises(ps, df2.tail(1).iloc[0].complete_question, 'C', model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5ff31ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hello world! it's been a long time since i've blogged. I'm not sure what's up, but i think i'll be writing more in the future.\\ni'm trying to get back into blogging, so i'm going to start with a post about my latest book, The Secret of the Fortune Wookiee. It's a book about the Star Wars universe and it's about the Fortune Wookiee.\\nThe\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_text('hello world!', max_new_tokens = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c63ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-19.2849, device='cuda:0'), tensor(-21.3479, device='cuda:0')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## I don't think this is very useful for evaluating \"belief\"\n",
    "# def text_logprob(text, model, norm = None):\n",
    "#     if not norm:\n",
    "#         norm = 1\n",
    "#     elif norm == \"whitespace\":\n",
    "#         norm = len(text.split())\n",
    "    \n",
    "#     logprobs = model.obs_logprobs(text)\n",
    "#     return [l.sum()/norm for l in logprobs] if type(logprobs)==list else logprobs.sum()/norm\n",
    "    \n",
    "        \n",
    "\n",
    "# [text_logprob(t, model, norm = \"whitespace\") for t in ['Some animals sweat in the heat to keep cool.', 'Sweat is a liquid that evaporates from the skin, which cools the body.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a654cdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sweat helps animals regulate their body temperature in hot environments', 'Sweat is a liquid that evaporates and cools the skin'], ['Animals use sweat to cool their bodies', 'Sweat contains water, which evaporates and cools the body'], ['Sweat is a liquid that helps regulate body temperature', 'Sweating helps animals cool down in hot environments'], ['Animals can secrete water from their skin to cool down', 'Water is a liquid'], ['Sweat is a liquid produced by the skin that helps regulate body temperature', 'Sweat helps animals stay cool in hot environments'], ['Animals use sweat to regulate body temperature', 'Sweat is a liquid that evaporates to cool the body'], ['Sweat is a liquid that helps regulate body temperature', 'Sweating helps animals adjust to hot environments'], ['Animals that live in hot and dry environments use sweat to cool their bodies', 'Sweat is a liquid that is secreted from the skin'], ['Sweat is a liquid produced by the skin that helps regulate body temperature', 'Sweat helps animals adjust to high temperatures'], ['Sweat is a liquid produced by the skin that helps regulate body temperature', 'Animals use sweat to cool themselves in hot environments']]\n",
      "tensor(1.3706, device='cuda:0')\n",
      "tensor(0.9371, device='cuda:0')\n",
      "tensor(1.3819, device='cuda:0')\n",
      "tensor(1.0045, device='cuda:0')\n",
      "tensor(1.3761, device='cuda:0')\n",
      "tensor(1.0208, device='cuda:0')\n",
      "tensor(1.6412, device='cuda:0')\n",
      "tensor(1.4444, device='cuda:0')\n",
      "tensor(1.6370, device='cuda:0')\n",
      "tensor(1.2777, device='cuda:0')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0eecc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9020, device='cuda:0')"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_premises(['Sweat is a liquid that comes from the skin of some animals.', \"Sweat helps animals adjust to hot environments.\"], df2.tail(1).iloc[0].complete_question, 'C', model)\n",
    "# out[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd7a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-15.9837,  -5.2983,  -9.4110], device='cuda:0')"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.logprobs('hello how are you today?')\n",
    "# tok_idx = x['tokens']['input_ids'].squeeze()\n",
    "\n",
    "logits = x['logprobs']\n",
    "\n",
    "logits[0, :, tok_idx[1:]].squeeze().diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240b31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some animals use a liquid coming from their skin to adjust to (A) cold (B) water (C) heat (D) humidity'"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.tail(1).iloc[0].complete_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4304d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1, 22172,   920,   526,   366,  9826, 29973], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-8.5676, device='cuda:0')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.tok.decode(tokens.squeeze()[1:])\n",
    "\n",
    "# tok_idx[1:]\n",
    "# logits[0,1:, tok_idx[1:]]\n",
    "print(tok_idx)\n",
    "# model.tok.decode(tok_idx[1:])\n",
    "logits[0][3][tok_idx[3]]\n",
    "\n",
    "## looks like, 0th place, look at token index for 1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a66a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['a','b','c'].index('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f76316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
