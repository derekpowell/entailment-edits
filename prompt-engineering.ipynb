{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6bdf34-8665-432b-a944-13ebe977aada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/dmpowell/.cache/huggingface\n",
      "/scratch/dmpowell/.cache/huggingface/datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "## ---------------------------------------------------------------------\n",
    "## set up configs for huggingface hub and OS paths on HPC cluster -- make sure config.ini is correct\n",
    "## ---------------------------------------------------------------------\n",
    "import configparser\n",
    "def auth_token():\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    return config[\"hugging_face\"][\"token\"]\n",
    "\n",
    "def scratch_path():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    return \"/scratch/\" + config[\"user\"][\"username\"] + \"/\"\n",
    "\n",
    "import os\n",
    "if os.path.isdir(scratch_path()):\n",
    "    os.environ['TRANSFORMERS_CACHE'] = scratch_path() + '.cache/huggingface'\n",
    "    os.environ['HF_DATASETS_CACHE'] = scratch_path() + '.cache/huggingface/datasets'\n",
    "print(os.getenv('TRANSFORMERS_CACHE'))\n",
    "print(os.getenv('HF_DATASETS_CACHE'))\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "## Load libraries\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from entailma import * ## these are where the QA and prompting functions live now\n",
    "from easyeditor.custom import EditedModel\n",
    "from easyeditor import LoRAHyperParams\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "## Ensure GPU is available -- device should == 'cuda'\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68833b8a-c4f5-48dd-8153-2c7ad99b7f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 17:45:27,824 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "08/09/2024 17:45:27 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004484653472900391,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 43,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024a3d54b86e4975b2d83d00aa95031c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "2024-08-09 17:46:34,483 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "08/09/2024 17:46:34 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to left...\n"
     ]
    }
   ],
   "source": [
    "## ---------------------------------------------------------------------\n",
    "## load llama-2 as a EditedModel class (not pipeline, to integrate better with other scripts/notebooks)\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\" \n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = LlamaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map = \"auto\")\n",
    "\n",
    "hparams = LoRAHyperParams.from_hparams('hparams/LoRA/llama-7b-canonical.yaml')\n",
    "model = EditedModel(hparams, auth_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a29eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question_stem</th>\n",
       "      <th>choices</th>\n",
       "      <th>complete_question</th>\n",
       "      <th>answer_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>9-743</td>\n",
       "      <td>where might a bunny live?</td>\n",
       "      <td>(A) a thicket (B) atop palm trees (C) a sewer ...</td>\n",
       "      <td>where might a bunny live? (A) a thicket (B) at...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>9-645</td>\n",
       "      <td>A shark will be unable to survive on eating al...</td>\n",
       "      <td>(A) it is a predator (B) it is a vegetarian (C...</td>\n",
       "      <td>A shark will be unable to survive on eating al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>8-250</td>\n",
       "      <td>A meadow vole just gave birth, and needs to fe...</td>\n",
       "      <td>(A) oil (B) deer (C) bugs (D) recycled plastic...</td>\n",
       "      <td>A meadow vole just gave birth, and needs to fe...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>283</td>\n",
       "      <td>The Grand Canyon was formed by</td>\n",
       "      <td>(A) a volcano erupting in 1782 (B) a river nam...</td>\n",
       "      <td>The Grand Canyon was formed by (A) a volcano e...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>8-183</td>\n",
       "      <td>A woman, with a pale complexion, wants to spen...</td>\n",
       "      <td>(A) UV rays are harmful (B) sunlight will be f...</td>\n",
       "      <td>A woman, with a pale complexion, wants to spen...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                      question_stem  \\\n",
       "490  9-743                          where might a bunny live?   \n",
       "491  9-645  A shark will be unable to survive on eating al...   \n",
       "492  8-250  A meadow vole just gave birth, and needs to fe...   \n",
       "493    283                     The Grand Canyon was formed by   \n",
       "494  8-183  A woman, with a pale complexion, wants to spen...   \n",
       "\n",
       "                                               choices  \\\n",
       "490  (A) a thicket (B) atop palm trees (C) a sewer ...   \n",
       "491  (A) it is a predator (B) it is a vegetarian (C...   \n",
       "492  (A) oil (B) deer (C) bugs (D) recycled plastic...   \n",
       "493  (A) a volcano erupting in 1782 (B) a river nam...   \n",
       "494  (A) UV rays are harmful (B) sunlight will be f...   \n",
       "\n",
       "                                     complete_question answer_key  \n",
       "490  where might a bunny live? (A) a thicket (B) at...          A  \n",
       "491  A shark will be unable to survive on eating al...          A  \n",
       "492  A meadow vole just gave birth, and needs to fe...          C  \n",
       "493  The Grand Canyon was formed by (A) a volcano e...          C  \n",
       "494  A woman, with a pale complexion, wants to spen...          A  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/obqa/test.tsv\", sep='\\t')\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df2 = df.copy().tail(10) # smaller df for testing\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fde841-b59e-4675-95ca-bd83bc1367cc",
   "metadata": {},
   "source": [
    "## ~~answer_questions()~~ mc_choose_answer() function\n",
    "\n",
    "This function will read a multiple choice question from the dataset and output a single letter response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f3f098e-cb23-49f8-9ee4-7ea3056e96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['model_answer'] = df2.apply(\n",
    "    lambda row: mc_choose_answer(row['complete_question'], model.model, model.tok),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4cc6000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum(df2[\"Answer Key\"] == df2[\"Model Answer\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc72891a",
   "metadata": {},
   "source": [
    "This is getting ~58% accuracy. For reference, the original GPT-3 with 32-shot examples got 65.8% ([Brown et al., 2020](https://arxiv.org/abs/2005.14165v4)). So that seems not-too-bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e2e8c-053d-4f9a-8e1d-3f125b4dc5ac",
   "metadata": {},
   "source": [
    "## generate_premises() function\n",
    "~~This function will read the model's statement from the data set and provide two premises that would make the statement true.~~\n",
    "\n",
    "UPDATE: This seems to work better if we include the original question and answer, which eliminates a point of failure and gives more context for the explanation / premise generation.\n",
    "\n",
    "UPDATE 2: This is in the `entailma` library in this repo, but I've reproduced it here to make it easier to play around with as you/we tweak prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "db215361-322c-42e9-a759-2ceb1c6d64c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Animals use sweat to cool their bodies', 'Sweat is a liquid that evaporates from the skin, which cools the body'], ['Sweat is a liquid that helps animals regulate body temperature', 'Animals use sweat to cool down when they get too hot'], ['Sweat is a liquid produced by the skin that helps regulate body temperature', 'Sweat is produced in response to heat and helps cool the body'], ['Animals use sweat glands to regulate body temperature', 'Sweat contains water and salt, which are lost through evaporation and help cool the body'], ['Animals use sweat to regulate body temperature', 'Sweat is a liquid produced by the skin'], ['Sweat is a liquid produced by the skin', 'Sweat helps animals regulate body temperature'], ['Animals use sweat to regulate their body temperature', 'Sweat is a liquid that is produced by the body to cool it down'], ['Sweat is a liquid that cools the body by evaporating off the skin', 'Sweat is produced in response to heat'], ['Sweat is a liquid produced by the skin to regulate body temperature', 'Sweat helps to cool the body by evaporating into the air'], ['Sweat is a liquid that helps regulate body temperature', 'Sweat evaporates and cools the body']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"entailma/prompt3b.txt\", 'r') as file:\n",
    "    premises_prompt = file.read()\n",
    "    \n",
    "\n",
    "def generate_premises(question, answer, model, tokenizer):\n",
    "    \n",
    "    input_str = f\"\\n\\n{premises_prompt}Question: {question}\\nAnswer: {answer}\\n\"\n",
    "\n",
    "    pipe = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        # device_map=\"cuda\",\n",
    "        device = model.device,\n",
    "        # use_auth_token = auth_token()\n",
    "    )\n",
    "\n",
    "    # sequences = pipe(\n",
    "    #     input_str,\n",
    "    #     do_sample=True,\n",
    "    #     top_p = .95,\n",
    "    #     # num_beams = 9, # beam search may be better ...\n",
    "    #     # num_beam_groups = 3,\n",
    "    #     max_new_tokens = 75,\n",
    "    #     temperature = 0.7,\n",
    "    #     num_return_sequences = 5\n",
    "    # )\n",
    "\n",
    "    sequences = pipe(\n",
    "        input_str,\n",
    "        penalty_alpha=0.9, \n",
    "        top_k=5,\n",
    "        max_new_tokens = 75,\n",
    "        num_return_sequences = 10\n",
    "    )\n",
    "    \n",
    "    generated_texts = [s['generated_text'] for s in sequences]\n",
    "    premises = [t[len(input_str):-1] for t in generated_texts]\n",
    "    premlist = [p.split(\".\\n\")[:2] for p in premises] \n",
    "\n",
    "    return premlist if len(premlist) > 1 else premlist[0]\n",
    "\n",
    "\n",
    "# df2['Generated Premises'] = df2.tail(1).copy().apply(\n",
    "#     lambda row: generate_premises(row['Complete Question'], row['Answer Key'], model.model, model.tok),\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "for row in df2.tail(1).itertuples():\n",
    "    out = generate_premises(row.complete_question, row.answer_key, model.model, model.tok)\n",
    "    print(out)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "69ec263d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Question: Electricity causes less damage to the Earth's atmosphere than (A) Gasoline (B) Potatoes (C) The sun (D) Water\\nAnswer: A\\nBurning gasoline releases pollutants.\\nPollutants damage the Earth's Atmosphere.\\n\\nQuestion: If a river is rushing southwest on a sunny day, then it is safe to assume that (A) southwest is a good place to be (B) the land gently inclines in that direction (C) the world is mostly land (D) the land is supple\\nAnswer: B\\nWater flows from a high point to a lower point.\\nThe direction of a river is the incline of the land.\\n\\nQuestion: Using a metal kitchen tool on a cheese can create (A) milk (B) blue cheese (C) melted cheese (D) small pieces\\nAnswer: D\\nMetal kitchen tools can cut food into small pieces.\\nCheese can be easily cut with metal kitchen tools.\\n\\nQuestion: Poison causes harm to which of the following? (A) a Tree (B) a robot (C) a house (D) a car\\nAnswer: A\\nPoison causes harm to living things.\\nA tree is a living thing.\\n\\nQuestion: There is most likely going to be fog around: (A) a marsh (B) a tundra (C) the plains (D) a desert\\nAnswer: A\\nFog forms in areas with high moisture.\\nA marsh has high moisture levels.\\n\\nQuestion: Predators eat (A) lions (B) humans (C) bunnies (D) grass\\nAnswer: C\\nPredators hunt and eat other animals.\\nBunnies are animals that predators hunt.\\n\\nQuestion: Oak tree seeds are planted and a sidewalk is paved right next to that spot, until eventually, the tree is tall and the roots must extend past the sidewalk, which means (A) roots may be split (B) roots may begin to die (C) parts may break the concrete (D) roots may fall apart\\nAnswer: C\\nTree roots grow and expand as the tree matures.\\nExpanding roots can push up and break through concrete surfaces.\\n\\nQuestion: As the rain forest is deforested the atmosphere will increase with (A) oxygen (B) nitrogen (C) carbon (D) rain\\nAnswer: C\\nTrees absorb carbon dioxide and store carbon.\\nDeforestation reduces the number of trees, releasing stored carbon into the atmosphere.\\n\\nQuestion: The main component in dirt is (A) microorganisms (B) broken stones (C) pollution (D) bacteria\\nAnswer: B\\nDirt is primarily composed of mineral particles.\\nMineral particles come from broken stones and rocks.\\n\\nQuestion: It's easier for human's to survive in: (A) a cave (B) the ocean. (C) a town ( D) alone\\nAnswer: C\\nTowns provide access to resources like food, water, and shelter.\\nTowns offer social support and services that aid in survival.\\n\\nQuestion: A cactus stem is used to store (A) fruit (B) liquid (C) food (D) spines\\nAnswer: B\\nCacti live in arid environments where water is scarce.\\nCactus stems store liquid to help the plant survive dry conditions.\\n\\nQuestion: The summer solstice in the northern hemisphere is four months before (A) May (B) July (C) April (D) October\\nAnswer: D\\nThe summer solstice in the northern hemisphere occurs in June.\\nFour months after June is October.\\n\\nQuestion: which of these would stop a car quicker? (A) a wheel with wet brake pads (B) a wheel without brake pads (C) a wheel with worn brake pads (D) a wheel with dry brake pads\\nAnswer: D\\nDry brake pads provide better friction against the wheel.\\nBetter friction allows for quicker stopping of the car.\\n\\nQuestion: The chance of wildfires is increased by (A) parched foliage (B) torrential rain (C) lush foliage (D) careful fire maintenance\\nAnswer: A\\nParched foliage is dry and highly flammable.\\nDry, flammable vegetation increases the likelihood of wildfires.\\n\\nQuestion: Overpopulation can cause (A) More fresh water for people to drink (B) Lower Life Expectancy in Countries (C) More food for more people (D) More space for places to people to live\\nAnswer: B\\nOverpopulation strains resources such as food, water, and healthcare.\\nStrained resources can lead to lower life expectancy in countries.\\n\\nQuestion: If you were attacked by a shark and had to punch it sharply where it pulls in air from, you'd use your hand to make contact with (A) its snout (B) its gills (C) its nose (D) its belly\\nAnswer: B\\nSharks pull in water through their gills for respiration.\\nPunching the gills can disrupt the shark's ability to breathe and deter the attack.\\n\\nQuestion: What system is needed for a body to get its needed supply of the gas humans breathe in? (A) the circulatory system (B) the digestive system (C) the school system (D) central nervous system\\nAnswer: A\\nThe circulatory system transports oxygen throughout the body.\\nOxygen is the gas humans breathe in and need for survival.\\n\\nQuestion: When it's flying, a plane has no friction with the (A) wings (B) ground (C) air (D) clouds\\nAnswer: B\\nFriction occurs when two surfaces are in contact.\\nA plane in flight is not in contact with the ground.\\n\\nQuestion: To grow plants require (A) acid rain (B) pesticides (C) shafts of sunlight (D) moonbeam rays\\nAnswer: C\\nPlants need sunlight for photosynthesis.\\nShafts of sunlight provide the light energy necessary for plants to grow.\\n\\nQuestion: What is the best way to guess a babies eye color? (A) The surroundings they are born in. (B) Their parents usual diet. (C) Just take a random guess. (D) The genealogy records of their family.\\nAnswer: D\\nEye color is inherited genetically.\\nGenealogy records can show the eye colors of the baby's ancestors.\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premises_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "59d76014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A person is heating water in order to cook pasta. He spills the pot of water on his leg and finds that the water (A) scalds (B) cools (C) toasts (D) freezes'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.iloc[5].complete_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c574dbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Soil is composed of microorganisms, mineral particles, and organic matter.\\n\\n[original] A tsunami is ']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reverse_statement(statement, model, tokenizer):\n",
    "    \n",
    "    with open(\"rephrase-prompt.txt\", 'r') as file:\n",
    "        reverse_prompt = file.read()\n",
    "    input_str = reverse_prompt + \"\\n\\n[original] \" + statement + '\\n[reversed]'\n",
    "    # print(input_str)\n",
    "\n",
    "    input_len = len(tokenizer(statement)['input_ids'])\n",
    "\n",
    "    pipe = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device = model.device,\n",
    "    )\n",
    "\n",
    "    sequences = pipe(\n",
    "        input_str,\n",
    "        # do_sample = False,\n",
    "        penalty_alpha=0.9, \n",
    "        top_k=5,\n",
    "        # num_beams = 5, # beam search may be better ...\n",
    "        max_new_tokens = input_len + 10\n",
    "    )\n",
    "    \n",
    "    generated_texts = [s['generated_text'] for s in sequences]\n",
    "    generation = [t[len(input_str):-1] for t in generated_texts]\n",
    "\n",
    "\n",
    "    return generation\n",
    "\n",
    "\n",
    "reverse_statement('Soil is composed of mineral particles, organic matter, and microorganisms.', model.model, model.tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3f687b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.3723, device='cuda:0')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seq_logprob(text, model):\n",
    "   # here: avged per token\n",
    "   return(model.completion_logprob(text, text) / len(model.tok(text)['input_ids']))\n",
    "\n",
    "seq_logprob('Soil is composed of mineral particles, and organic matter', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1877c272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.choose(mc_answer_prompt + '\\n\\nQuestion: A person is heating water in order to cook pasta. He spills the pot of water on his leg and finds that the water (A) scalds (B) cools (C) toasts (D) freezes\\nAnswer:', choices = [\"A\", \"B\", \"C\", \"D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e07a8b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# somehow this doesn't work?\n",
    "\n",
    "model.tok.decode(model.tok('hello world')['input_ids'][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "bfcb6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def premise_logprob():\n",
    "def logprobs(text, model):\n",
    "    x = model.logprobs(text)\n",
    "    tok_idx = x['tokens']['input_ids'].squeeze()\n",
    "    logprobs = x['logprobs']\n",
    "\n",
    "    return logprobs[0, :, tok_idx[1:]].squeeze().diag()\n",
    "\n",
    "\n",
    "def comps_logprobs(text, comp_list, model):\n",
    "    comp_idx = model.tok(comp_list)\n",
    "    x = model.logprobs(text)\n",
    "    tok_idx = x['tokens']['input_ids'].squeeze()\n",
    "    logprobs = x['logprobs']\n",
    "\n",
    "    logprobs[0, :, comp_list]\n",
    "\n",
    "\n",
    "\n",
    "def seq_logprob(text, model, norm = False):\n",
    "   return logprobs(text, model).sum() if not norm else logprobs(text, model).sum()/(len(model.tok(text)['input_ids']) - 1)\n",
    "\n",
    "\n",
    "seq_logprob('The sound a dog makes is ', model)\n",
    "\n",
    "# x['logprobs'][]\n",
    "\n",
    "# def seq_logprob(text, model):\n",
    "#    # here: avged per token\n",
    "#    return(model.completion_logprob(text, text) / len(model.tok(text)['input_ids']))\n",
    "\n",
    "\n",
    "# Seems like question would be, degree to which is changes odds of original answer vs other answers\n",
    "# but need this also to work for cases where orig answer was right ?\n",
    "\n",
    "# def validate_premises():\n",
    "# def validate_premises(premises, question, answer, model):\n",
    "   \n",
    "#    logit_0 = tok_logprobs(mc_answer_prompt +  \"\\n\\nQuestion:\" + question+\"\\nAnswer: \" + answer, model)[-1]\n",
    "\n",
    "#    premise_str = \"\\n\".join(premises)\n",
    "#    logit_1 = tok_logprobs(mc_answer_prompt + '\\n\\n' + premise_str + '\\nQuestion: ' + question  + 'Answer: ' + answer, model)[-1]\n",
    "\n",
    "#    # return odds ratio: odd_before (target vs others) / odds_after(target vs others)\n",
    "#    return(logit_0, logit_1)\n",
    "\n",
    "\n",
    "# validate_premises(['Some things are red.', 'A fair number of things in the world are red.'], 'What color is the sky? (A) blue (B) red (C) yellow (D) black', 'B', model)\n",
    "# append the premises ahead of the (raw) question and check change in odds/probability of specified answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "1f95ce1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8646, -2.9641, -3.7612, -3.6786], device='cuda:0')"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mc_choose_answer(question, model, tokenizer=None):\n",
    "    if not tokenizer:\n",
    "        tokenizer = model.tok\n",
    "    \n",
    "    input_str = mc_answer_prompt + f\"\\nQuestion: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    sequences = model.generate(input_ids = input_ids, max_new_tokens = 1)\n",
    "    \n",
    "    return tokenizer.decode(sequences[0])[-1]\n",
    "\n",
    "\n",
    "def last_token_logprobs(text, last_tokens, model):\n",
    "    x = model.logprobs(text)\n",
    "    logprobs = x['logprobs']\n",
    "    t_idx = [i[-1] for i in model.tok(last_tokens)['input_ids']]\n",
    "\n",
    "    return(logprobs[0, -1, t_idx])\n",
    "\n",
    "\n",
    "def mc_answer_logprobs(question, model, answers = ['A','B','C','D']):\n",
    "\n",
    "    input_str = mc_answer_prompt + f\"\\n\\nQuestion: {question}\\nAnswer: \"\n",
    "\n",
    "    return last_token_logprobs(input_str, answers, model)\n",
    "\n",
    "\n",
    "mc_answer_logprobs('What color is the sky? (A) blue (B) red (C) orange (D) black', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920cc16e",
   "metadata": {},
   "source": [
    "## updates:\n",
    "\n",
    "\n",
    "- Need a way to score whether the premises are actually any \"good\" -- i.e. do they lead the model to choose the targeted answer? The code below implements an IKE/ICE-style version of this. It seems to work ok?\n",
    "- Need to add more examples to the prompt of premises supportin INCORRECT answers, as it struggles with this ATM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "371a7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_premises(premises, question, target_answer, model, answers = ['A','B','C','D']):\n",
    "\n",
    "   reg_answer_prompt = mc_answer_prompt +  \"\\n\\nQuestion:\" + question+\"\\nAnswer: \"\n",
    "   logprobs0 = last_token_logprobs(reg_answer_prompt, answers, model)\n",
    "   prob0 = logprobs0[answers.index(target_answer)].exp() / logprobs0.exp().sum()\n",
    "\n",
    "   premise_str = \"\\n\".join(premises)\n",
    "   augmented_answer_prompt = mc_answer_prompt + '\\n\\n' + premise_str + '\\nQuestion: ' + question  + 'Answer: '\n",
    "   logprobs1 = last_token_logprobs(augmented_answer_prompt,  answers, model)\n",
    "   prob1 = logprobs1[answers.index(target_answer)].exp() / logprobs1.exp().sum()\n",
    "\n",
    "   return( (prob1/(1-prob1)) / (prob0/(1-prob0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "044fd57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4303, device='cuda:0')\n",
      "tensor(1.5171, device='cuda:0')\n",
      "tensor(1.2276, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# The first are very good premises that reinforce the target answer, the latter are not!\n",
    "\n",
    "print(score_premises(['The sky is red.', 'At sunset, the sun can be extremely red.'], 'What color is the sky? (A) blue (B) red (C) yellow (D) black', 'B', model))\n",
    "print(score_premises(['Some things are red.', 'My favorite color is red.'], 'What color is the sky? (A) blue (B) red (C) yellow (D) black', 'B', model))\n",
    "print(score_premises(['red red red red.', 'red red red red red.'], 'What color is the sky? (A) blue (B) red (C) yellow (D) black', 'B', model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "ad21df75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some animals use a liquid coming from their skin to adjust to (A) cold (B) water (C) heat (D) humidity'"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.tail(1).iloc[0].complete_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "a654cdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sweat helps animals regulate their body temperature in hot environments', 'Sweat is a liquid that evaporates and cools the skin'], ['Animals use sweat to cool their bodies', 'Sweat contains water, which evaporates and cools the body'], ['Sweat is a liquid that helps regulate body temperature', 'Sweating helps animals cool down in hot environments'], ['Animals can secrete water from their skin to cool down', 'Water is a liquid'], ['Sweat is a liquid produced by the skin that helps regulate body temperature', 'Sweat helps animals stay cool in hot environments'], ['Animals use sweat to regulate body temperature', 'Sweat is a liquid that evaporates to cool the body'], ['Sweat is a liquid that helps regulate body temperature', 'Sweating helps animals adjust to hot environments'], ['Animals that live in hot and dry environments use sweat to cool their bodies', 'Sweat is a liquid that is secreted from the skin'], ['Sweat is a liquid produced by the skin that helps regulate body temperature', 'Sweat helps animals adjust to high temperatures'], ['Sweat is a liquid produced by the skin that helps regulate body temperature', 'Animals use sweat to cool themselves in hot environments']]\n",
      "tensor(1.3706, device='cuda:0')\n",
      "tensor(0.9371, device='cuda:0')\n",
      "tensor(1.3819, device='cuda:0')\n",
      "tensor(1.0045, device='cuda:0')\n",
      "tensor(1.3761, device='cuda:0')\n",
      "tensor(1.0208, device='cuda:0')\n",
      "tensor(1.6412, device='cuda:0')\n",
      "tensor(1.4444, device='cuda:0')\n",
      "tensor(1.6370, device='cuda:0')\n",
      "tensor(1.2777, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for row in df2.tail(1).itertuples():\n",
    "    out = generate_premises(row.complete_question, 'C', model.model, model.tok)\n",
    "    print(out)\n",
    "\n",
    "for ps in out:\n",
    "    print(score_premises(ps, df2.tail(1).iloc[0].complete_question, 'C', model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "8f0eecc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9020, device='cuda:0')"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_premises(['Sweat is a liquid that comes from the skin of some animals.', \"Sweat helps animals adjust to hot environments.\"], df2.tail(1).iloc[0].complete_question, 'C', model)\n",
    "# out[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "2a6c333d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4208, 0.1950, 0.1903, 0.1939]),\n",
       " tensor([0.3358, 0.2574, 0.2057, 0.2010]))"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([-6.3389, -7.1079, -7.1321, -7.1137]).exp() / torch.tensor([-6.3389, -7.1079, -7.1321, -7.1137]).exp().sum(), torch.tensor([-5.1819, -5.4479, -5.6720, -5.6951]).exp() / torch.tensor([-5.1819, -5.4479, -5.6720, -5.6951]).exp().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "f2cd7a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-15.9837,  -5.2983,  -9.4110], device='cuda:0')"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.logprobs('hello how are you today?')\n",
    "# tok_idx = x['tokens']['input_ids'].squeeze()\n",
    "\n",
    "logits = x['logprobs']\n",
    "\n",
    "logits[0, :, tok_idx[1:]].squeeze().diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "a240b31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some animals use a liquid coming from their skin to adjust to (A) cold (B) water (C) heat (D) humidity'"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.tail(1).iloc[0].complete_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4c4304d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1, 22172,   920,   526,   366,  9826, 29973], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-8.5676, device='cuda:0')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.tok.decode(tokens.squeeze()[1:])\n",
    "\n",
    "# tok_idx[1:]\n",
    "# logits[0,1:, tok_idx[1:]]\n",
    "print(tok_idx)\n",
    "# model.tok.decode(tok_idx[1:])\n",
    "logits[0][3][tok_idx[3]]\n",
    "\n",
    "## looks like, 0th place, look at token index for 1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "540a66a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['a','b','c'].index('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f76316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
