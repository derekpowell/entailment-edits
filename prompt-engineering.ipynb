{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6bdf34-8665-432b-a944-13ebe977aada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/dmpowell/.cache/huggingface\n",
      "/scratch/dmpowell/.cache/huggingface/datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "## ---------------------------------------------------------------------\n",
    "## set up configs for huggingface hub and OS paths on HPC cluster -- make sure config.ini is correct\n",
    "## ---------------------------------------------------------------------\n",
    "import configparser\n",
    "def auth_token():\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    return config[\"hugging_face\"][\"token\"]\n",
    "\n",
    "def scratch_path():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    return \"/scratch/\" + config[\"user\"][\"username\"] + \"/\"\n",
    "\n",
    "import os\n",
    "if os.path.isdir(scratch_path()):\n",
    "    os.environ['TRANSFORMERS_CACHE'] = scratch_path() + '.cache/huggingface'\n",
    "    os.environ['HF_DATASETS_CACHE'] = scratch_path() + '.cache/huggingface/datasets'\n",
    "print(os.getenv('TRANSFORMERS_CACHE'))\n",
    "print(os.getenv('HF_DATASETS_CACHE'))\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "## Load libraries\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from entailma import * ## these are where the QA and prompting functions live now\n",
    "from easyeditor.custom import EditedModel\n",
    "from easyeditor import LoRAHyperParams, FTHyperParams, BaseEditor\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "## Ensure GPU is available -- device should == 'cuda'\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f6c525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedModel:\n",
    "    def __init__(self, model, tokenizer, auth_token=None):\n",
    "        \n",
    "        self.model = model\n",
    "        self.tok = tokenizer\n",
    "        self.tok.pad_token_id = self.tok.eos_token_id\n",
    "        # self.model_name = self.editor.model_name\n",
    "\n",
    "        # self.params = hparams\n",
    "        self.preprompt = \"\"\n",
    "        self.saved_weights = None\n",
    "        \n",
    "        if type(self.tok) == transformers.LlamaTokenizer or transformers.LlamaTokenizerFast:\n",
    "            self.tok.padding_side = \"right\"\n",
    "        else: \n",
    "            self.tok.padding_side = \"left\"\n",
    "    \n",
    "    def edit(self, rewrite, log_file = None, **kwargs):\n",
    "        if log_file:\n",
    "            h = open(log_file, \"a\")\n",
    "        else:\n",
    "            h = None\n",
    "        \n",
    "        if \"preprompt\" in rewrite: # this is a little hacky\n",
    "            self.preprompt = rewrite[\"preprompt\"]\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            with redirect_stdout(h): # None\n",
    "                metrics, self.model, self.saved_weights = self.editor.pure_edit( # pure_edit\n",
    "                    **rewrite,\n",
    "                    # **kwargs,\n",
    "                    keep_original_weight = True,\n",
    "                    verbose = False\n",
    "                )\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def restore(self):\n",
    "\n",
    "        self.preprompt = \"\"\n",
    "        \n",
    "        if self.params.alg_name == \"LoRA\":\n",
    "            self.model = self.model.unload()\n",
    "        \n",
    "        elif self.saved_weights:\n",
    "\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    for k, v in self.saved_weights.items():\n",
    "                        nethook.get_parameter(self.model, k)[...] = v\n",
    "                self.saved_weights = None\n",
    "                # print(\"Original model restored\")\n",
    "            except NameError as e:\n",
    "                print(f\"No model weights to restore: {e}\")\n",
    "\n",
    "        elif self.saved_weights == {}:\n",
    "            print (print(f\"No model weights to restore: saved_weights is empty dict\"))\n",
    "\n",
    "        return None\n",
    "\n",
    "            \n",
    "    def generate_text(self, texts, **kwargs):\n",
    "        \n",
    "        if type(texts) != list:\n",
    "            texts = [texts]\n",
    "        \n",
    "        texts = [self.preprompt + t for t in texts]\n",
    "\n",
    "        model = self.model\n",
    "        tokenizer = self.tok\n",
    "        encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**encoding, **kwargs) # \n",
    "\n",
    "            generated_texts = tokenizer.batch_decode(\n",
    "                generated_ids, skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "        return(generated_texts)\n",
    "    \n",
    "    \n",
    "    # def logprobs(self, texts):\n",
    "        \n",
    "    #     # texts = self.preprompt + texts if type(texts)==str else [self.preprompt + t for t in texts]\n",
    "    \n",
    "    #     # tokenizer = self.tok \n",
    "    #     # model = self.model\n",
    "    #     # encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "    #     # with torch.no_grad():\n",
    "    #     #     model_out = model(encoding[\"input_ids\"])\n",
    "    #     #     logits = model_out.logits\n",
    "    #     #     logprobs = F.log_softmax(logits, -1)\n",
    "\n",
    "    #     x = self.logits(texts)\n",
    "        \n",
    "    #     return {\"tokens\": x['tokens'], \"logprobs\": logprobs}\n",
    "    \n",
    "\n",
    "    def logits(self, texts):\n",
    "        \n",
    "        texts = self.preprompt + texts if type(texts)==str else [self.preprompt + t for t in texts]\n",
    "    \n",
    "        tokenizer = self.tok \n",
    "        model = self.model\n",
    "        encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_out = model(encoding[\"input_ids\"])\n",
    "            logits = model_out.logits\n",
    "        \n",
    "        return {\"tokens\": encoding, \"logits\": logits}\n",
    "    \n",
    "    \n",
    "    def logprobs(self, texts):\n",
    "        \n",
    "        logits = self.logits(texts)\n",
    "        \n",
    "        return {\"tokens\": logits['tokens'], \"logprobs\": F.log_softmax(logits['logits'], -1)}\n",
    "    \n",
    "    \n",
    "    def obs_logits(self, text):\n",
    "    \n",
    "        x = self.logits(text)\n",
    "        logits = x['logits']\n",
    "        \n",
    "        obslogits = []\n",
    "\n",
    "        if type(text) is str:\n",
    "            tok_idx = x['tokens']['input_ids'].squeeze()\n",
    "            logits = x['logits']\n",
    "            obslogits = logits[0, :, tok_idx[1:]].squeeze().diag()\n",
    "\n",
    "        elif type(text) is list:\n",
    "            for i in range(len(text)):\n",
    "                tok_idx = x['tokens']['input_ids'][i].squeeze()\n",
    "                mask = x['tokens']['attention_mask'][i] > 0\n",
    "                \n",
    "                obslogits.append(logits[0, :, tok_idx[1:]].squeeze().diag()[mask[1:]])\n",
    "\n",
    "        return obslogits\n",
    "\n",
    "\n",
    "    def obs_logprobs(self, text):\n",
    "        logits = self.obs_logits(text)\n",
    "\n",
    "        return [F.log_softmax(l, -1) for l in logits] if type(logits)==list else F.log_softmax(logits, -1)\n",
    "        \n",
    "       \n",
    "    def completion_logprob(self, text, completion, start_ind = None):\n",
    "        \n",
    "        '''\n",
    "        Compute model log probability of completion substring. Returns single value tensor. Takes only one text string.\n",
    "        '''\n",
    "\n",
    "        return self.substring_logprobs(text, completion)[0][-1]\n",
    "        \n",
    "\n",
    "    def substring_logprobs(self, texts, substring, pad = True):\n",
    "        '''\n",
    "        Compute model log probability of each occurrence of substring in text. Returns list of list-type. Accepts a list of strings.\n",
    "        '''\n",
    "        \n",
    "        if type(texts) != list:\n",
    "            texts = [texts]\n",
    "        \n",
    "        logprobs = self.logprobs(texts)\n",
    "        \n",
    "        tok_encoded = encode_token(substring, self.tok, pad = pad)\n",
    "        # text_encoded = logprobs['tokens']['input_ids'][0].tolist()\n",
    "        \n",
    "        out = []\n",
    "        for i in range(len(texts)):\n",
    "            text_encoded = logprobs['tokens']['input_ids'][i].tolist()\n",
    "\n",
    "            # find matches for searched token sequence\n",
    "            start_idxs = []\n",
    "            for left in range(0, len(text_encoded) - len(tok_encoded)+1):\n",
    "                # left = i - 1\n",
    "                right = left + len(tok_encoded)\n",
    "                if text_encoded[left:right] == tok_encoded:\n",
    "                    start_idxs.append(left)\n",
    "\n",
    "            lp = logprobs['logprobs'][i]\n",
    "            match_probs = []\n",
    "\n",
    "            # compute probability for all tokens\n",
    "            for start in start_idxs:\n",
    "                val = 0\n",
    "                for i in range(len(tok_encoded)):\n",
    "                    val += lp[start + i - 1][tok_encoded[i]]\n",
    "                match_probs.append(val)\n",
    "\n",
    "            out.append(match_probs)\n",
    "\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def choose(self, prompt, choices, normalization = None):\n",
    "\n",
    "        # prompt = prompt.rstrip() # remove any trailing whitespace\n",
    "\n",
    "        if type(self.tok) == transformers.models.llama.tokenization_llama.LlamaTokenizer:\n",
    "            padded_choices = choices\n",
    "            prompt = prompt + \" \" if prompt[-1]!= \" \" else prompt\n",
    "        else:\n",
    "            padded_choices = [pad_token(c) for c in choices] # pad all the \n",
    "        \n",
    "        prompts = [prompt + c for c in padded_choices]\n",
    "\n",
    "        logits = torch.tensor([self.completion_logprob(prompts[i], padded_choices[i]) for i in range(len(padded_choices))])\n",
    "\n",
    "        if normalization == \"unconditional\":\n",
    "            norm_logits = torch.tensor([self.completion_logprob(padded_choices[i], padded_choices[i]) for i in range(len(padded_choices))])\n",
    "            logits = logits - norm_logits\n",
    "\n",
    "        elif normalization == \"byte_length\":    \n",
    "            str_lens = [len(c) for c in choices]\n",
    "            logits = logits / torch.tensor(str_lens)\n",
    "\n",
    "        elif normalization == \"token_length\":\n",
    "            tok_lens = [len(encode_token(c, self.tok)) for c in choices]\n",
    "            logits = logits / torch.tensor(tok_lens)\n",
    "\n",
    "        elif normalization == \"root\":\n",
    "            tok_lens = [len(encode_token(c, self.tok)) for c in choices]\n",
    "            logits = torch.pow(torch.exp(logits), 1./torch.tensor(tok_lens))\n",
    "\n",
    "        logits = logits.tolist()\n",
    "\n",
    "        return(logits.index(max(logits)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68833b8a-c4f5-48dd-8153-2c7ad99b7f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/11/2024 09:25:33 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004864215850830078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 73,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e065e5f776df492b8c59647f43f5a41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "## ---------------------------------------------------------------------\n",
    "## load llama-2 as a EditedModel class (not pipeline, to integrate better with other scripts/notebooks)\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\" \n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = LlamaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map = \"auto\")\n",
    "model = WrappedModel(\n",
    "    LlamaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map = \"auto\"),\n",
    "    LlamaTokenizer.from_pretrained(MODEL_NAME)\n",
    ")\n",
    "\n",
    "# hparams = FTHyperParams.from_hparams('hparams/FT/llama-7b.yaml')\n",
    "# model = EditedModel(hparams, auth_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a29eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question_stem</th>\n",
       "      <th>choices</th>\n",
       "      <th>complete_question</th>\n",
       "      <th>answer_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>9-743</td>\n",
       "      <td>where might a bunny live?</td>\n",
       "      <td>(A) a thicket (B) atop palm trees (C) a sewer ...</td>\n",
       "      <td>where might a bunny live? (A) a thicket (B) at...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>9-645</td>\n",
       "      <td>A shark will be unable to survive on eating al...</td>\n",
       "      <td>(A) it is a predator (B) it is a vegetarian (C...</td>\n",
       "      <td>A shark will be unable to survive on eating al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>8-250</td>\n",
       "      <td>A meadow vole just gave birth, and needs to fe...</td>\n",
       "      <td>(A) oil (B) deer (C) bugs (D) recycled plastic...</td>\n",
       "      <td>A meadow vole just gave birth, and needs to fe...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>283</td>\n",
       "      <td>The Grand Canyon was formed by</td>\n",
       "      <td>(A) a volcano erupting in 1782 (B) a river nam...</td>\n",
       "      <td>The Grand Canyon was formed by (A) a volcano e...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>8-183</td>\n",
       "      <td>A woman, with a pale complexion, wants to spen...</td>\n",
       "      <td>(A) UV rays are harmful (B) sunlight will be f...</td>\n",
       "      <td>A woman, with a pale complexion, wants to spen...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                      question_stem  \\\n",
       "490  9-743                          where might a bunny live?   \n",
       "491  9-645  A shark will be unable to survive on eating al...   \n",
       "492  8-250  A meadow vole just gave birth, and needs to fe...   \n",
       "493    283                     The Grand Canyon was formed by   \n",
       "494  8-183  A woman, with a pale complexion, wants to spen...   \n",
       "\n",
       "                                               choices  \\\n",
       "490  (A) a thicket (B) atop palm trees (C) a sewer ...   \n",
       "491  (A) it is a predator (B) it is a vegetarian (C...   \n",
       "492  (A) oil (B) deer (C) bugs (D) recycled plastic...   \n",
       "493  (A) a volcano erupting in 1782 (B) a river nam...   \n",
       "494  (A) UV rays are harmful (B) sunlight will be f...   \n",
       "\n",
       "                                     complete_question answer_key  \n",
       "490  where might a bunny live? (A) a thicket (B) at...          A  \n",
       "491  A shark will be unable to survive on eating al...          A  \n",
       "492  A meadow vole just gave birth, and needs to fe...          C  \n",
       "493  The Grand Canyon was formed by (A) a volcano e...          C  \n",
       "494  A woman, with a pale complexion, wants to spen...          A  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/obqa/test.tsv\", sep='\\t')\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df2 = df.copy().tail(10) # smaller df for testing\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f95ce1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8643, -2.9570, -3.7617, -3.6758], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def mc_choose_answer(question, model, tokenizer=None):\n",
    "#     if not tokenizer:\n",
    "#         tokenizer = model.tok\n",
    "    \n",
    "#     input_str = mc_answer_prompt + f\"\\nQuestion: {question}\\nAnswer:\"\n",
    "#     inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "#     input_ids = inputs[\"input_ids\"].cuda()\n",
    "#     with torch.no_grad():\n",
    "#         sequences = model.generate(input_ids = input_ids, max_new_tokens = 1)\n",
    "    \n",
    "#     return tokenizer.decode(sequences[0])[-1]\n",
    "\n",
    "\n",
    "def last_token_logprobs(text, last_tokens, model):\n",
    "    x = model.logprobs(text)\n",
    "    logprobs = x['logprobs']\n",
    "    t_idx = [i[-1] for i in model.tok(last_tokens)['input_ids']]\n",
    "\n",
    "    return(logprobs[0, -1, t_idx])\n",
    "\n",
    "\n",
    "def mc_answer_logprobs(question, model, answers = ['A','B','C','D']):\n",
    "\n",
    "    input_str = mc_answer_prompt + f\"\\n\\nQuestion: {question}\\nAnswer: \"\n",
    "\n",
    "    return last_token_logprobs(input_str, answers, model)\n",
    "\n",
    "\n",
    "mc_answer_logprobs('What color is the sky? (A) blue (B) red (C) orange (D) black', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc72891a",
   "metadata": {},
   "source": [
    "Question answering is getting ~58% accuracy. For reference, the original GPT-3 with 32-shot examples got 65.8% ([Brown et al., 2020](https://arxiv.org/abs/2005.14165v4)). So that seems not-too-bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e2e8c-053d-4f9a-8e1d-3f125b4dc5ac",
   "metadata": {},
   "source": [
    "## generate_premises() function\n",
    "~~This function will read the model's statement from the data set and provide two premises that would make the statement true.~~\n",
    "\n",
    "UPDATE: This seems to work better if we include the original question and answer, which eliminates a point of failure and gives more context for the explanation / premise generation.\n",
    "\n",
    "UPDATE 2: This is in the `entailma` library in this repo, but I've reproduced it here to make it easier to play around with as you/we tweak prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920cc16e",
   "metadata": {},
   "source": [
    "## updates:\n",
    "\n",
    "\n",
    "- Need a way to score whether the premises are actually any \"good\" -- i.e. do they lead the model to choose the targeted answer? The code below implements an IKE/ICE-style version of this. It seems to work ok?\n",
    "- Need to add more examples to the prompt of premises supportin INCORRECT answers, as it struggles with this ATM [quick and dirty version done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "371a7d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1445, device='cuda:0', dtype=torch.float16)\n",
      "tensor(1.8545, device='cuda:0', dtype=torch.float16)\n",
      "tensor(1.2861, device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "def completion_prob(preprompt, question, target_answer, model, answers = ['A','B','C','D']):\n",
    "   if len(preprompt) == 0:\n",
    "      prompt = mc_answer_prompt +  \"\\n\\n\" + preprompt + \"Question:\" + question + \"\\nAnswer: \"\n",
    "   else:\n",
    "      prompt = mc_answer_prompt + '\\n\\n' + preprompt + '\\nQuestion: ' + question  + '\\nAnswer: '\n",
    "   \n",
    "   logprobs0 = last_token_logprobs(prompt, answers, model)\n",
    "   prob = logprobs0[answers.index(target_answer)].exp() / logprobs0.exp().sum()\n",
    "\n",
    "   return prob\n",
    "\n",
    "\n",
    "\n",
    "# def score_premises(premises, question, target_answer, model, answers = ['A','B','C','D']):\n",
    "#    '''Returns the odds-ratio of the target answer with vs without the premises in the premises in the context.'''\n",
    "#    reg_answer_prompt = mc_answer_prompt +  \"\\n\\nQuestion:\" + question+\"\\nAnswer: \"\n",
    "#    logprobs0 = last_token_logprobs(reg_answer_prompt, answers, model)\n",
    "#    prob0 = logprobs0[answers.index(target_answer)].exp() / logprobs0.exp().sum()\n",
    "   \n",
    "\n",
    "#    premise_str = \"\\n\".join(premises)\n",
    "#    augmented_answer_prompt = mc_answer_prompt + '\\n\\n' + premise_str + '\\nQuestion: ' + question  + 'Answer: '\n",
    "#    logprobs1 = last_token_logprobs(augmented_answer_prompt,  answers, model)\n",
    "#    prob1 = logprobs1[answers.index(target_answer)].exp() / logprobs1.exp().sum()\n",
    "\n",
    "#    return (prob1/(1-prob1)) / (prob0/(1-prob0))\n",
    "\n",
    "\n",
    "def score_premises(premises, question, target_answer, model, base_prob = None, answers = ['A','B','C','D']):\n",
    "   '''Returns the odds-ratio of the target answer with vs without the premises in the premises in the context.'''\n",
    "   \n",
    "   if not base_prob:\n",
    "      base_prob = completion_prob(\"\", question, target_answer, model, answers)\n",
    "\n",
    "   premise_str = \"\\n\".join(premises)\n",
    "   prob1 = completion_prob(premise_str, question, target_answer, model, answers)\n",
    "\n",
    "   return( (prob1/(1-prob1)) / (base_prob/(1-base_prob)))\n",
    "\n",
    "\n",
    "print(score_premises(['The sky is red.', 'At sunset, the sun can be extremely red.'], 'What color is the sky? (A) blue (B) red (C) yellow (D) black', 'B', model))\n",
    "print(score_premises(['Some things are red.', 'My favorite color is red.'], 'What color is the sky? (A) blue (B) red (C) yellow (D) black', 'B', model))\n",
    "print(score_premises(['red red red red.', 'red red red red red.'], 'What color is the sky? (A) blue (B) red (C) yellow (D) black', 'B', model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "db215361-322c-42e9-a759-2ceb1c6d64c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Some animals use mucus produced by their skin to adjust to cold.', 'Mucus is a protective liquid produced by animals to protect against cold.'], tensor(4.3242, device='cuda:0', dtype=torch.float16))\n"
     ]
    }
   ],
   "source": [
    "# 32-shot prompt (mix of test and dev so we should fix this later)\n",
    "with open(\"entailma/entailer-dev-prompt-tandf.txt\", 'r') as file:\n",
    "    premises_prompt = file.read()\n",
    "    \n",
    "\n",
    "def check_repeat_words(text1, text2, max_repeat_size = 4):\n",
    "    # check if text2 includes a repetition of more than max_repeat_size in a row\n",
    "\n",
    "    t1 = text1.lower().split()\n",
    "    t2 = text2.lower().split()\n",
    "\n",
    "    sublists = []\n",
    "    for idx in range(len(t1) - max_repeat_size+1):\n",
    "        s = t1[idx:idx+max_repeat_size+1]\n",
    "        if len(s) > max_repeat_size:\n",
    "            sublists.append(' '.join(s))\n",
    "    \n",
    "    sublists2 = []\n",
    "    for idx in range(len(t2) - max_repeat_size+1):\n",
    "        s = t2[idx:idx+max_repeat_size+1]\n",
    "        if len(s) > max_repeat_size:\n",
    "            sublists2.append(' '.join(s))\n",
    "    \n",
    "    valid = True\n",
    "\n",
    "    for seq in sublists2:\n",
    "        if seq in sublists:\n",
    "            valid = False\n",
    "            break\n",
    "\n",
    "    return(valid)\n",
    "\n",
    "\n",
    "def generate_premises(question, answer, model, num_prem = 1, batch_size = 4):\n",
    "    \n",
    "    input_str = f\"{premises_prompt}\\n\\nQuestion: {question}\\nAnswer: {answer}\\n\"\n",
    "\n",
    "    pipe = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model = model.model,\n",
    "        tokenizer = model.tok,\n",
    "        torch_dtype=torch.float16,\n",
    "        device = model.model.device\n",
    "    )\n",
    "\n",
    "    seq_list = []\n",
    "    for i in range(-(-num_prem // batch_size)):\n",
    "\n",
    "        sequences = pipe(\n",
    "            input_str,\n",
    "            do_sample = True,\n",
    "            top_k = 50,\n",
    "            penalty_alpha = 0.6, # avoids repetition of the question + answer (except doesn't)\n",
    "            temperature = 0.7,\n",
    "            max_new_tokens = 50,\n",
    "            num_return_sequences = min(batch_size, num_prem - i*batch_size)\n",
    "        )\n",
    "\n",
    "        seq_list += sequences\n",
    "    \n",
    "    generated_texts = [s['generated_text'] for s in seq_list]\n",
    "    \n",
    "    premises = [t[len(input_str):-1] for t in generated_texts]\n",
    "    premlist = [p.split(\"\\n\")[:2] for p in premises] \n",
    "\n",
    "    return premlist if len(premlist) > 1 else premlist[0]\n",
    "\n",
    "\n",
    "def generate_best_premises(question, answer, model, num_prem=10, batch_size = 4):\n",
    "    premises = generate_premises(question, answer, model, num_prem)\n",
    "    premise_validity = [check_repeat_words(question, '\\n'.join(p), 5) for p in premises]\n",
    "\n",
    "    valid_premises = [i for (i, v) in zip(premises, premise_validity) if v]\n",
    "\n",
    "    if len(valid_premises) > 0:\n",
    "\n",
    "        base_completion_prob = completion_prob(\"\", question, answer, model)\n",
    "        scores =  [score_premises(p, question, answer, model, base_prob = base_completion_prob) for p in valid_premises]\n",
    "        max_idx = scores.index(max(scores))\n",
    "        \n",
    "        return valid_premises[max_idx], scores[max_idx]\n",
    "    \n",
    "    else:\n",
    "        return [\"\",\"\"], -100.\n",
    "\n",
    "\n",
    "row = df2.iloc[-1]\n",
    "out = generate_best_premises(row.complete_question, 'A', model, num_prem = 32, batch_size = 8)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d692b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "46c63ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## I don't think this is very useful for evaluating \"belief\"\n",
    "# def text_logprob(text, model, norm = None):\n",
    "#     if not norm:\n",
    "#         norm = 1\n",
    "#     elif norm == \"whitespace\":\n",
    "#         norm = len(text.split())\n",
    "    \n",
    "#     logprobs = model.obs_logprobs(text)\n",
    "#     return [l.sum()/norm for l in logprobs] if type(logprobs)==list else logprobs.sum()/norm\n",
    "    \n",
    "# [text_logprob(t, model, norm = \"whitespace\") for t in ['Some animals sweat in the heat to keep cool.', 'Sweat is a liquid that evaporates from the skin, which cools the body.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "733536ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
