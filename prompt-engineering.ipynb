{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6bdf34-8665-432b-a944-13ebe977aada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/dmpowell/.cache/huggingface\n",
      "/scratch/dmpowell/.cache/huggingface/datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/data/grp_dmpowell/.mamba/envs/EasyEditShared/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "## ---------------------------------------------------------------------\n",
    "## set up configs for huggingface hub and OS paths on HPC cluster -- make sure config.ini is correct\n",
    "## ---------------------------------------------------------------------\n",
    "import configparser\n",
    "def auth_token():\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    return config[\"hugging_face\"][\"token\"]\n",
    "\n",
    "def scratch_path():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    return \"/scratch/\" + config[\"user\"][\"username\"] + \"/\"\n",
    "\n",
    "import os\n",
    "if os.path.isdir(scratch_path()):\n",
    "    os.environ['TRANSFORMERS_CACHE'] = scratch_path() + '.cache/huggingface'\n",
    "    os.environ['HF_DATASETS_CACHE'] = scratch_path() + '.cache/huggingface/datasets'\n",
    "print(os.getenv('TRANSFORMERS_CACHE'))\n",
    "print(os.getenv('HF_DATASETS_CACHE'))\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "## Load libraries\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from entailma import * ## these are where the QA and prompting functions live now\n",
    "from easyeditor.custom import EditedModel\n",
    "from easyeditor import LoRAHyperParams\n",
    "\n",
    "## ---------------------------------------------------------------------\n",
    "## Ensure GPU is available -- device should == 'cuda'\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68833b8a-c4f5-48dd-8153-2c7ad99b7f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 17:45:27,824 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "08/09/2024 17:45:27 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004484653472900391,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 43,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024a3d54b86e4975b2d83d00aa95031c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "2024-08-09 17:46:34,483 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "08/09/2024 17:46:34 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to left...\n"
     ]
    }
   ],
   "source": [
    "## ---------------------------------------------------------------------\n",
    "## load llama-2 as a EditedModel class (not pipeline, to integrate better with other scripts/notebooks)\n",
    "## ---------------------------------------------------------------------\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\" \n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = LlamaForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map = \"auto\")\n",
    "\n",
    "hparams = LoRAHyperParams.from_hparams('hparams/LoRA/llama-7b-canonical.yaml')\n",
    "model = EditedModel(hparams, auth_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a29eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question_stem</th>\n",
       "      <th>choices</th>\n",
       "      <th>complete_question</th>\n",
       "      <th>answer_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>9-743</td>\n",
       "      <td>where might a bunny live?</td>\n",
       "      <td>(A) a thicket (B) atop palm trees (C) a sewer ...</td>\n",
       "      <td>where might a bunny live? (A) a thicket (B) at...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>9-645</td>\n",
       "      <td>A shark will be unable to survive on eating al...</td>\n",
       "      <td>(A) it is a predator (B) it is a vegetarian (C...</td>\n",
       "      <td>A shark will be unable to survive on eating al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>8-250</td>\n",
       "      <td>A meadow vole just gave birth, and needs to fe...</td>\n",
       "      <td>(A) oil (B) deer (C) bugs (D) recycled plastic...</td>\n",
       "      <td>A meadow vole just gave birth, and needs to fe...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>283</td>\n",
       "      <td>The Grand Canyon was formed by</td>\n",
       "      <td>(A) a volcano erupting in 1782 (B) a river nam...</td>\n",
       "      <td>The Grand Canyon was formed by (A) a volcano e...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>8-183</td>\n",
       "      <td>A woman, with a pale complexion, wants to spen...</td>\n",
       "      <td>(A) UV rays are harmful (B) sunlight will be f...</td>\n",
       "      <td>A woman, with a pale complexion, wants to spen...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                      question_stem  \\\n",
       "490  9-743                          where might a bunny live?   \n",
       "491  9-645  A shark will be unable to survive on eating al...   \n",
       "492  8-250  A meadow vole just gave birth, and needs to fe...   \n",
       "493    283                     The Grand Canyon was formed by   \n",
       "494  8-183  A woman, with a pale complexion, wants to spen...   \n",
       "\n",
       "                                               choices  \\\n",
       "490  (A) a thicket (B) atop palm trees (C) a sewer ...   \n",
       "491  (A) it is a predator (B) it is a vegetarian (C...   \n",
       "492  (A) oil (B) deer (C) bugs (D) recycled plastic...   \n",
       "493  (A) a volcano erupting in 1782 (B) a river nam...   \n",
       "494  (A) UV rays are harmful (B) sunlight will be f...   \n",
       "\n",
       "                                     complete_question answer_key  \n",
       "490  where might a bunny live? (A) a thicket (B) at...          A  \n",
       "491  A shark will be unable to survive on eating al...          A  \n",
       "492  A meadow vole just gave birth, and needs to fe...          C  \n",
       "493  The Grand Canyon was formed by (A) a volcano e...          C  \n",
       "494  A woman, with a pale complexion, wants to spen...          A  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/obqa/test.tsv\", sep='\\t')\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df2 = df.copy().tail(10) # smaller df for testing\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fde841-b59e-4675-95ca-bd83bc1367cc",
   "metadata": {},
   "source": [
    "## ~~answer_questions()~~ mc_choose_answer() function\n",
    "\n",
    "This function will read a multiple choice question from the dataset and output a single letter response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f3f098e-cb23-49f8-9ee4-7ea3056e96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['model_answer'] = df2.apply(\n",
    "    lambda row: mc_choose_answer(row['complete_question'], model.model, model.tok),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4cc6000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum(df2[\"Answer Key\"] == df2[\"Model Answer\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc72891a",
   "metadata": {},
   "source": [
    "This is getting ~58% accuracy. For reference, the original GPT-3 with 32-shot examples got 65.8% ([Brown et al., 2020](https://arxiv.org/abs/2005.14165v4)). So that seems not-too-bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e2e8c-053d-4f9a-8e1d-3f125b4dc5ac",
   "metadata": {},
   "source": [
    "## generate_premises() function\n",
    "~~This function will read the model's statement from the data set and provide two premises that would make the statement true.~~\n",
    "\n",
    "UPDATE: This seems to work better if we include the original question and answer, which eliminates a point of failure and gives more context for the explanation / premise generation.\n",
    "\n",
    "UPDATE 2: This is in the `entailma` library in this repo, but I've reproduced it here to make it easier to play around with as you/we tweak prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "db215361-322c-42e9-a759-2ceb1c6d64c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Water boils at 100 degrees Celsius', 'Water that is 100 degrees Celsius is hot and can cause a burn']\n",
      "['Water bubbles when it is heated', 'Water bubbles when it is heated']\n",
      "['Diseases can be prevented by vaccinations and healthy lifestyles', 'A decrease in diseases leads to less sick people']\n",
      "['Soil is made up of mineral particles and organic matter', 'Soil is not made up of living things']\n",
      "['Sweat is a liquid produced by the skin that helps to cool the body', 'Animals living in hot environments sweat to regulate their body temperature']\n"
     ]
    }
   ],
   "source": [
    "with open(\"entailma/prompt3b.txt\", 'r') as file:\n",
    "    premises_prompt = file.read()\n",
    "    \n",
    "\n",
    "def generate_premises(question, answer, model, tokenizer):\n",
    "    \n",
    "    input_str = f\"\\n\\n{premises_prompt}Question: {question}\\nAnswer: {answer}\\n\"\n",
    "\n",
    "    pipe = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        # device_map=\"cuda\",\n",
    "        device = model.device,\n",
    "        # use_auth_token = auth_token()\n",
    "    )\n",
    "\n",
    "    # sequences = pipe(\n",
    "    #     input_str,\n",
    "    #     do_sample=True,\n",
    "    #     top_p = .95,\n",
    "    #     # num_beams = 9, # beam search may be better ...\n",
    "    #     # num_beam_groups = 3,\n",
    "    #     max_new_tokens = 75,\n",
    "    #     temperature = 0.7,\n",
    "    #     num_return_sequences = 5\n",
    "    # )\n",
    "\n",
    "    sequences = pipe(\n",
    "        input_str,\n",
    "        penalty_alpha=0.9, \n",
    "        top_k=5,\n",
    "        max_new_tokens = 75,\n",
    "        num_return_sequences = 1\n",
    "    )\n",
    "    \n",
    "    generated_texts = [s['generated_text'] for s in sequences]\n",
    "    premises = [t[len(input_str):-1] for t in generated_texts]\n",
    "    premlist = [p.split(\".\\n\")[:2] for p in premises] \n",
    "\n",
    "    return premlist if len(premlist) > 1 else premlist[0]\n",
    "\n",
    "\n",
    "# df2['Generated Premises'] = df2.tail(1).copy().apply(\n",
    "#     lambda row: generate_premises(row['Complete Question'], row['Answer Key'], model.model, model.tok),\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "for row in df2.tail(5).itertuples():\n",
    "    out = generate_premises(row.complete_question, row.answer_key, model.model, model.tok)\n",
    "    print(out)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "69ec263d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Question: Electricity causes less damage to the Earth's atmosphere than (A) Gasoline (B) Potatoes (C) The sun (D) Water\\nAnswer: A\\nBurning gasoline releases pollutants.\\nPollutants damage the Earth's Atmosphere.\\n\\nQuestion: If a river is rushing southwest on a sunny day, then it is safe to assume that (A) southwest is a good place to be (B) the land gently inclines in that direction (C) the world is mostly land (D) the land is supple\\nAnswer: B\\nWater flows from a high point to a lower point.\\nThe direction of a river is the incline of the land.\\n\\nQuestion: Using a metal kitchen tool on a cheese can create (A) milk (B) blue cheese (C) melted cheese (D) small pieces\\nAnswer: D\\nMetal kitchen tools can cut food into small pieces.\\nCheese can be easily cut with metal kitchen tools.\\n\\nQuestion: Poison causes harm to which of the following? (A) a Tree (B) a robot (C) a house (D) a car\\nAnswer: A\\nPoison causes harm to living things.\\nA tree is a living thing.\\n\\nQuestion: There is most likely going to be fog around: (A) a marsh (B) a tundra (C) the plains (D) a desert\\nAnswer: A\\nFog forms in areas with high moisture.\\nA marsh has high moisture levels.\\n\\nQuestion: Predators eat (A) lions (B) humans (C) bunnies (D) grass\\nAnswer: C\\nPredators hunt and eat other animals.\\nBunnies are animals that predators hunt.\\n\\nQuestion: Oak tree seeds are planted and a sidewalk is paved right next to that spot, until eventually, the tree is tall and the roots must extend past the sidewalk, which means (A) roots may be split (B) roots may begin to die (C) parts may break the concrete (D) roots may fall apart\\nAnswer: C\\nTree roots grow and expand as the tree matures.\\nExpanding roots can push up and break through concrete surfaces.\\n\\nQuestion: As the rain forest is deforested the atmosphere will increase with (A) oxygen (B) nitrogen (C) carbon (D) rain\\nAnswer: C\\nTrees absorb carbon dioxide and store carbon.\\nDeforestation reduces the number of trees, releasing stored carbon into the atmosphere.\\n\\nQuestion: The main component in dirt is (A) microorganisms (B) broken stones (C) pollution (D) bacteria\\nAnswer: B\\nDirt is primarily composed of mineral particles.\\nMineral particles come from broken stones and rocks.\\n\\nQuestion: It's easier for human's to survive in: (A) a cave (B) the ocean. (C) a town ( D) alone\\nAnswer: C\\nTowns provide access to resources like food, water, and shelter.\\nTowns offer social support and services that aid in survival.\\n\\nQuestion: A cactus stem is used to store (A) fruit (B) liquid (C) food (D) spines\\nAnswer: B\\nCacti live in arid environments where water is scarce.\\nCactus stems store liquid to help the plant survive dry conditions.\\n\\nQuestion: The summer solstice in the northern hemisphere is four months before (A) May (B) July (C) April (D) October\\nAnswer: D\\nThe summer solstice in the northern hemisphere occurs in June.\\nFour months after June is October.\\n\\nQuestion: which of these would stop a car quicker? (A) a wheel with wet brake pads (B) a wheel without brake pads (C) a wheel with worn brake pads (D) a wheel with dry brake pads\\nAnswer: D\\nDry brake pads provide better friction against the wheel.\\nBetter friction allows for quicker stopping of the car.\\n\\nQuestion: The chance of wildfires is increased by (A) parched foliage (B) torrential rain (C) lush foliage (D) careful fire maintenance\\nAnswer: A\\nParched foliage is dry and highly flammable.\\nDry, flammable vegetation increases the likelihood of wildfires.\\n\\nQuestion: Overpopulation can cause (A) More fresh water for people to drink (B) Lower Life Expectancy in Countries (C) More food for more people (D) More space for places to people to live\\nAnswer: B\\nOverpopulation strains resources such as food, water, and healthcare.\\nStrained resources can lead to lower life expectancy in countries.\\n\\nQuestion: If you were attacked by a shark and had to punch it sharply where it pulls in air from, you'd use your hand to make contact with (A) its snout (B) its gills (C) its nose (D) its belly\\nAnswer: B\\nSharks pull in water through their gills for respiration.\\nPunching the gills can disrupt the shark's ability to breathe and deter the attack.\\n\\nQuestion: What system is needed for a body to get its needed supply of the gas humans breathe in? (A) the circulatory system (B) the digestive system (C) the school system (D) central nervous system\\nAnswer: A\\nThe circulatory system transports oxygen throughout the body.\\nOxygen is the gas humans breathe in and need for survival.\\n\\nQuestion: When it's flying, a plane has no friction with the (A) wings (B) ground (C) air (D) clouds\\nAnswer: B\\nFriction occurs when two surfaces are in contact.\\nA plane in flight is not in contact with the ground.\\n\\nQuestion: To grow plants require (A) acid rain (B) pesticides (C) shafts of sunlight (D) moonbeam rays\\nAnswer: C\\nPlants need sunlight for photosynthesis.\\nShafts of sunlight provide the light energy necessary for plants to grow.\\n\\nQuestion: What is the best way to guess a babies eye color? (A) The surroundings they are born in. (B) Their parents usual diet. (C) Just take a random guess. (D) The genealogy records of their family.\\nAnswer: D\\nEye color is inherited genetically.\\nGenealogy records can show the eye colors of the baby's ancestors.\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premises_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "59d76014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A person is heating water in order to cook pasta. He spills the pot of water on his leg and finds that the water (A) scalds (B) cools (C) toasts (D) freezes'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.iloc[5].complete_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c574dbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Soil is composed of microorganisms, mineral particles, and organic matter.\\n\\n[original] A tsunami is ']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reverse_statement(statement, model, tokenizer):\n",
    "    \n",
    "    with open(\"rephrase-prompt.txt\", 'r') as file:\n",
    "        reverse_prompt = file.read()\n",
    "    input_str = reverse_prompt + \"\\n\\n[original] \" + statement + '\\n[reversed]'\n",
    "    # print(input_str)\n",
    "\n",
    "    input_len = len(tokenizer(statement)['input_ids'])\n",
    "\n",
    "    pipe = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device = model.device,\n",
    "    )\n",
    "\n",
    "    sequences = pipe(\n",
    "        input_str,\n",
    "        # do_sample = False,\n",
    "        penalty_alpha=0.9, \n",
    "        top_k=5,\n",
    "        # num_beams = 5, # beam search may be better ...\n",
    "        max_new_tokens = input_len + 10\n",
    "    )\n",
    "    \n",
    "    generated_texts = [s['generated_text'] for s in sequences]\n",
    "    generation = [t[len(input_str):-1] for t in generated_texts]\n",
    "\n",
    "\n",
    "    return generation\n",
    "\n",
    "\n",
    "reverse_statement('Soil is composed of mineral particles, organic matter, and microorganisms.', model.model, model.tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3f687b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.3723, device='cuda:0')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seq_logprob(text, model):\n",
    "   # here: avged per token\n",
    "   return(model.completion_logprob(text, text) / len(model.tok(text)['input_ids']))\n",
    "\n",
    "seq_logprob('Soil is composed of mineral particles, and organic matter', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1877c272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.choose(mc_answer_prompt + '\\n\\nQuestion: A person is heating water in order to cook pasta. He spills the pot of water on his leg and finds that the water (A) scalds (B) cools (C) toasts (D) freezes\\nAnswer:', choices = [\"A\", \"B\", \"C\", \"D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e07a8b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# somehow this doesn't work?\n",
    "\n",
    "model.tok.decode(model.tok('hello world')['input_ids'][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "bfcb6b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.9128, device='cuda:0')"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def premise_logprob():\n",
    "def tok_logprobs(text, model, start_ind = 1):\n",
    "   x = model.logprobs(text)\n",
    "   tok_idx = x['tokens']['input_ids'].squeeze()\n",
    "   logits = x['logprobs']\n",
    "\n",
    "   return logits[0, :, tok_idx[1:]].squeeze().diag()\n",
    "\n",
    "def seq_logprob(text, model, start_ind = 1, norm = False):\n",
    "   return tok_logprobs(text, model, start_ind).sum() if not norm else tok_logprobs(text, model).sum()/(len(model.tok(text)['input_ids']) - 1)\n",
    "\n",
    "seq_logprob('dogs meow', model, norm = True)\n",
    "\n",
    "# x['logprobs'][]\n",
    "\n",
    "# def seq_logprob(text, model):\n",
    "#    # here: avged per token\n",
    "#    return(model.completion_logprob(text, text) / len(model.tok(text)['input_ids']))\n",
    "\n",
    "\n",
    "\n",
    "# def validate_premises():\n",
    "# def validate_premises(premises, question, answer, model):\n",
    "#    premise_str = \"\\n\".join(premises)\n",
    "#    input_str =  \"Question:\" + question+\"\\n\" + premise_str + \"Answer:\"\n",
    "#    model.completion_logprob(input_str + )\n",
    "\n",
    "# append the premises ahead of the (raw) question and check change in odds/probability of specified answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1f95ce1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-13.2433, device='cuda:0')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_logprobs('hello world', model).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f2cd7a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12.0732,  -6.5442,  -0.8319,  -0.1216,  -2.8828,  -1.2245],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.logprobs('hello how are you today?')\n",
    "tok_idx = x['tokens']['input_ids'].squeeze()\n",
    "logits = x['logprobs']\n",
    "\n",
    "logits[0, :, tok_idx[1:]].squeeze().diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4c4304d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1, 22172,   920,   526,   366,  9826, 29973], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-8.5676, device='cuda:0')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.tok.decode(tokens.squeeze()[1:])\n",
    "\n",
    "# tok_idx[1:]\n",
    "# logits[0,1:, tok_idx[1:]]\n",
    "print(tok_idx)\n",
    "# model.tok.decode(tok_idx[1:])\n",
    "logits[0][3][tok_idx[3]]\n",
    "\n",
    "## looks like, 0th place, look at token index for 1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a66a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
