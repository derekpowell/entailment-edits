alg_name: "LoRA_ga"
model_name: "meta-llama/Llama-2-7b-hf"
device: 1
lora_type: "lora"
layers: []
num_steps: 20 #70
batch_size: 16 #1
max_length: 30
lr: 5e-3
weight_decay: 0
kl_factor: 0
rank: 1
lora_alpha: 1
lora_dropout: 0.1
norm_constraint: false
target_modules: '.*\.(14|15|16)\.mlp\.(down_proj|up_proj|gate_proj)' #'.*\.(4|5|6|7)\.mlp\.(down_proj|up_proj|gate_proj)' 
model_parallel: true